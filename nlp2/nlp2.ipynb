{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                               2η Εργαστηριακή Άσκηση \n",
    "### Ονοματεπώνυμο : Σιφναίος Σάββας\n",
    "### ΑΜ : 031 16080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Θεωρητικό Υπόβραθρο \n",
    "\n",
    "Κατά την εκπαίδευση ενός συστήματος αναγνώρισης φώνης, κρίνεται απαραίτητη η εξαγωγή ορισμένων χαρακτηριστικών από το σήμα φωνής. Στο συγκεκριμένο εργαστήριο, για την απεικόνιση των χαρακτηριστικών αυτών θα χρησιμοποιήσουμε τα MFCC Vectors. Η παραπάνω ιδέα των MFCC βασίζεται αφενός σε αρχές και έννοιες της θεωρίας του cepstrum και αφετέρου σε αποτελέσματα ψυχοακουστικών μελετών που πραγματοποιήθηκαν για τον εντοπισμό των συχνοτικών εκεινών ζωνών που γίνονται \"ευκολότερα\"  αντιληπτές από τον ανθρώπινο αυτί. Σκόπος των μελετών αυτών, είναι βέλτιστη κωδικοποιήση των ηχητικών σημάτων.\n",
    "\n",
    "Η εξαγωγή των χαρακτηριστικών των ηχητικών σημάτων εισόδου, μπορεί να οργανωθεί στα εξής βήματα : \n",
    "\n",
    "1) Προέμφαση του ήχου μέσα από την μετατόπιση κάθε φωνήματος σε υψηλότερες συχνότητες που βοηθούν την αναγνώριση.\n",
    "\n",
    "2) Παραθυροποίηση των ηχητικών δεδομένων για να διατηρήσουμε την στατικότητα των στατιστικών χαρακτηριστικών του ήχου(μέση τιμή, διακύμανση) και κατ’ επέκταση την εργοδικότητα (αφού έχουμε πλήθος δεδομένων στο χρόνο αλλά δε μπορούμε να επαναλάβουμε διαδοχικά τις ίδιες εκτελέσεις των ίδιων δεδομένων).Συχνά χρησιμοποιούμε Kaiser ή Hamming παράθυρα ,διάρκειας από 15ms εως 30ms, με δεδομένο overlap (περίπου 40-60%) για να ομαλοποιηθούν φαινόμενα των άκρων.\n",
    "\n",
    "3) Συχνοτική ανάλυση σήματος ,μέσω της χρήσης DFT, για τον υπολογισμό της συγκέντρωσης της ενέργειας σε κάθε συχνότητα.\n",
    "\n",
    "4) Mel filter bank διαμόρφωση,που αιτιολογείται επειδή ο ανθρώπινος εγκέφαλος παρουσιάζει δυσκολία στον διαχωρισμό συχνοτήτων σε υψηλοσυχνοτικά σήματα.Γενικά η ανθρώπινη απόκριση είναι γραμμική κάτω από τα 1000Hz και λογαριθμική παραπάνω,οπότε έτσι σχεδιάονται και τα φίλτρα μας.\n",
    "\n",
    "5) Χρήση cepstrum, για την μετάθεση του σήματος σε πεδίο quefrency και το διαχωρισμό πηγής και φίλτρου. Αποδεικνύεται ότι ο διαχωρισμός αυτός βελτιώνει το μοντέλο καθώς ο εντοπισμός π.χ. του φίλτρου: Θέση της γλώσσας μπορεί να προσφέρει στην ανάλυση παραπάνω χαρακτηριστικά για την αναγνώριση των φωνημάτων.\n",
    "\n",
    "6) Delta-Training: Τα χαρακτηριστικά αυτά είναι τιμές που δείχνουν πόσο μεταβάλλονται κάποια στατιστικά χαρακτηριστικά του cepstrum μέσα σε ένα παράθυρο. Συμπληρώνουν το διάνυσμα που δημιουργούμε.Για ακρίβεια χρησιμοποιούμε και τα double-deltas που δείχνουν την μεταβολή των deltas . Αυτός ο χειρισμός του σήματος καταλήγει σε MFC Coeficients και περιέχει το μεγαλύτερο μέρος της πληροφορίας για κάθε παραθυροποιημένο σήμα.\n",
    "\n",
    "• Γλωσσικό Μοντέλο (Language Model LM). Για το μοντέλο αυτό συνήθως χρησιμοποιούμε unigrams και bigrams. Ωστόσο, κατά την χρησιμοποίηση του Kaldi για την δημιουργία του μοντέλου αναγνώρισης φωνής βλέπουμε ότι δημιουργεί unigram, bigram συνδυάζοντάς τα και προφανώς αυξάνοντας την υπολογιστική πολυπλοκότητα αλλά και την ακρίβεια όσο αυξάνεται το μέγεθος του μοντέλου.\n",
    "\n",
    "• Ακουστικό Μοντέλο (Acoustic Model AM). Στο στάδιο της δημιουργίας του ακουστικού μοντέλου ουσιαστικά υπολογίζουμε την πιο πιθανή ακολουθία από παρατηρήσεις όταν μας έχουν δοθεί άλλα γλωσσικά χαρακτηριστικά όπως (λέξεις, φωνήματα ή κάποια μέρη φωνημάτων). Πιο συγκεκριμένα δοθέντος μίας πιθανότητας P(O|W) μπορούμε να κατατάξουμε για κάθε HMM μίας κατάστασης q του αυτομάτου μας, χρησιμοποιώντας Gaussian Mixture Models (GMM’s) και τα υπόλοιπα γλωσσικά χαρακτηριστικά, την πιθανοφάνεια."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.    Βήματα Προπαρασκεύης \n",
    "\n",
    "Αρχικά, στο φάκελο egs, δημιουργούμε ένα νέο φάκελο με όνομα 'usc'. Στον φάκελο 'usc', δημιουργούμε τον φάκελο 'data'.Στην συνέχεια, μέσα σε αυτό το φάκελο δημιουργούμε τους υποφακέλους 'train', 'test' , 'dev' στους οποίους θα αποθηκέυσουμε στην συνέχεια τα αντίστοιχα αρχεία 'uttids' , 'utt2spk' , 'wav.scp' , 'text' ,που θα χρησιμοποιηθούν για την κατασκευή του γλωσσικού και του ακουστικού μοντέλου."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: /users/savas/kaldi/egs/usc: File exists\n",
      "mkdir: data: File exists\n",
      "mkdir: data/train: File exists\n",
      "mkdir: data/test: File exists\n",
      "mkdir: data/dev: File exists\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\nmkdir /users/savas/kaldi/egs/usc\\ncd /users/savas/kaldi/egs/usc\\nmkdir data\\nmkdir data/train \\nmkdir data/test\\nmkdir data/dev\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bfae462ba06f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nmkdir /users/savas/kaldi/egs/usc\\ncd /users/savas/kaldi/egs/usc\\nmkdir data\\nmkdir data/train \\nmkdir data/test\\nmkdir data/dev\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</Users/Savas/opt/anaconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\nmkdir /users/savas/kaldi/egs/usc\\ncd /users/savas/kaldi/egs/usc\\nmkdir data\\nmkdir data/train \\nmkdir data/test\\nmkdir data/dev\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "mkdir /users/savas/kaldi/egs/usc\n",
    "cd /users/savas/kaldi/egs/usc\n",
    "mkdir data\n",
    "mkdir data/train \n",
    "mkdir data/test\n",
    "mkdir data/dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "global_path = \"/Users/Savas/kaldi/egs/usc/data/\"\n",
    "train_path = global_path +\"train\"\n",
    "test_path = global_path +\"test\"\n",
    "dev_path = global_path +\"dev\"\n",
    "paths = {'train':train_path,'test':test_path,'dev':dev_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Πριν προχωρήσουμε στην δημιουργία των παραπάνω αρχείων, πραγματοποιούμε μια μορφή προεπεξεργάσιας των δεδομένων μας, τα οποια βρίσκονται αποθηκευμένα στον φάκελο 'slp_lab2_data'. Για τα αρχεία 'train_utterances.txt' , 'test_utterances.txt' , 'validation_utterances.txt' , προβαίνουμε σε αντικατάσταση των συμβόλων \"_\" με το      \" \" (κενό) και το αποτέλεσμα της αντικατάστασης αυτής το αποθηκεύουμε στα αρχεία 'train_utt.txt', 'test_utt.txt', 'val_utt.txt' , αντίστοιχα. Τα αρχεία αυτά αποτέλουν μια προσωρινή , ευκολότερα επεξεργάσιμη μορφή των αρχείων που τα δημιούργησαν. Στην σύνεχεια, από την ενδιάμεση αυτή μορφή απομονώνουμε το τρίτο πεδίο και το αποθηκεύουμε στα αρχεία 'train_utter.txt' , 'test_utter.txt' , 'val_utter.txt'. Τα τελευταία τρία αρχεία, έχουν αποθηκευμένα σε κάθε γραμμή τον ομιλητή που εκφωνεί την συγκεκριμένη πρόταση στο αντίστοιχο set αρχείων. Επιπλέον, στα αρχεία 'sample_train_utter.txt' , 'sample_test_utter.txt' , 'sample_val_utter.txt', αποθηκεύουμε το τέταρτο πεδίο από τα αρχεία 'train_utt.txt', 'test_utt.txt', 'val_utt.txt' αντίστοιχα. Στα τελευταία αυτά αρχεία, περιέχεται ,ανά γραμμή, ο αριθμός κάθε πρότασης για κάθε ένα από τα τρία σύνολα δεδομένων. Τέλος, επεξεργαζόμαστε κατάλληλα και το αρχείο 'lexicon.txt'. Συγκεκριμένα, απομονώνουμε το πρώτο και το δεύτερο πεδίο του αρχείου αυτού αποθηκεύοντας τα πεδία αυτά στα αρχεία 'indices_lexicon.txt' , 'temp.txt' αντίστοιχα. Το αρχείο 'temp.txt' θα ήταν κατάλληλο για χρήση στην συνέχεια, ωστόσο στην αρχή κάθε γραμμής περιέχει ένα χαρακτήρα κενού, ο οποίος ενδεχομένως να προκαλέσει προβλήματα στην συνέχεια. Για τον λόγο αυτό, με την εντολή 'cut -b 2- ,απορρίπτουμε το πρώτο byte κάθε γραμμής (που αντιστοιχεί στον χαρακτήρα του κενού) και το αποτέλεσμα το αποθηκεύουμε στο αρχείο 'values_lexicon.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 3: ./slp_lab2_data/filesets/train_utt.txt: No such file or directory\n",
      "cat: ./slp_lab2_data/filesets/train_utterances.txt: No such file or directory\n",
      "bash: line 4: ./slp_lab2_data/filesets/test_utt.txt: No such file or directory\n",
      "cat: ./slp_lab2_data/filesets/test_utterances.txt: No such file or directory\n",
      "bash: line 5: ./slp_lab2_data/filesets/val_utt.txt: No such file or directory\n",
      "cat: ./slp_lab2_data/filesets/validation_utterances.txt: No such file or directory\n",
      "bash: line 8: ./slp_lab2_data/indices_lexicon.txt: No such file or directory\n",
      "bash: line 9: ./slp_lab2_data/temp.txt: No such file or directory\n",
      "bash: line 11: ./slp_lab2_data/values_lexicon.txt: No such file or directory\n",
      "bash: line 14: ./slp_lab2_data/filesets/train_utter.txt: No such file or directory\n",
      "bash: line 15: ./slp_lab2_data/filesets/test_utter.txt: No such file or directory\n",
      "bash: line 16: ./slp_lab2_data/filesets/val_utter.txt: No such file or directory\n",
      "bash: line 19: ./slp_lab2_data/filesets/sample_train_utter.txt: No such file or directory\n",
      "bash: line 20: ./slp_lab2_data/filesets/sample_test_utter.txt: No such file or directory\n",
      "bash: line 21: ./slp_lab2_data/filesets/sample_val_utter.txt: No such file or directory\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\n#Replace \"_\" with spaces and save the output to \\'*_utt.txt\\' files \\ncat ./slp_lab2_data/filesets/train_utterances.txt |tr \"_\" [:space:] > ./slp_lab2_data/filesets/train_utt.txt \\ncat ./slp_lab2_data/filesets/test_utterances.txt |tr \"_\" [:space:] > ./slp_lab2_data/filesets/test_utt.txt\\ncat ./slp_lab2_data/filesets/validation_utterances.txt |tr \"_\" [:space:] > ./slp_lab2_data/filesets/val_utt.txt\\n\\n#From \\'lexicon.txt\\' we save the first and second field in seperate files\\ncut -f 1 ./slp_lab2_data/lexicon.txt > ./slp_lab2_data/indices_lexicon.txt\\ncut -f 2 ./slp_lab2_data/lexicon.txt > ./slp_lab2_data/temp.txt\\n#From the file containing the second field of \\'lexicon.txt\\' we save every byte from the second one till the last one \\ncut -b 2- ./slp_lab2_data/temp.txt > ./slp_lab2_data/values_lexicon.txt\\n\\n#Create new \\'*_utter.txt\\' files containing only the 3rd field of \\'*_utt.txt\\' files accordingly\\ncut -f 3 ./slp_lab2_data/filesets/train_utt.txt > ./slp_lab2_data/filesets/train_utter.txt \\ncut -f 3 ./slp_lab2_data/filesets/test_utt.txt > ./slp_lab2_data/filesets/test_utter.txt \\ncut -f 3 ./slp_lab2_data/filesets/val_utt.txt > ./slp_lab2_data/filesets/val_utter.txt \\n\\n#Create new \\'*_utter.txt\\' files containing only the 4th field of \\'*_utt.txt\\' files accordingly\\ncut -f 4 ./slp_lab2_data/filesets/train_utt.txt > ./slp_lab2_data/filesets/sample_train_utter.txt \\ncut -f 4 ./slp_lab2_data/filesets/test_utt.txt > ./slp_lab2_data/filesets/sample_test_utter.txt \\ncut -f 4 ./slp_lab2_data/filesets/val_utt.txt > ./slp_lab2_data/filesets/sample_val_utter.txt \\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-33c7b3f9ffff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n#Replace \"_\" with spaces and save the output to \\'*_utt.txt\\' files \\ncat ./slp_lab2_data/filesets/train_utterances.txt |tr \"_\" [:space:] > ./slp_lab2_data/filesets/train_utt.txt \\ncat ./slp_lab2_data/filesets/test_utterances.txt |tr \"_\" [:space:] > ./slp_lab2_data/filesets/test_utt.txt\\ncat ./slp_lab2_data/filesets/validation_utterances.txt |tr \"_\" [:space:] > ./slp_lab2_data/filesets/val_utt.txt\\n\\n#From \\'lexicon.txt\\' we save the first and second field in seperate files\\ncut -f 1 ./slp_lab2_data/lexicon.txt > ./slp_lab2_data/indices_lexicon.txt\\ncut -f 2 ./slp_lab2_data/lexicon.txt > ./slp_lab2_data/temp.txt\\n#From the file containing the second field of \\'lexicon.txt\\' we save every byte from the second one till the last one \\ncut -b 2- ./slp_lab2_data/temp.txt > ./slp_lab2_data/values_lexicon.txt\\n\\n#Create new \\'*_utter.txt\\' files containing only the 3rd field of \\'*_utt.txt\\' files accordingly\\ncut -f 3 ./slp_lab2_data/filesets/train_utt.txt > ./slp_lab2_data/filesets/train_utter.txt \\ncut -f 3 ./slp_lab2_data/filesets/test_utt.txt > ./slp_lab2_data/filesets/test_utter.txt \\ncut -f 3 ./slp_lab2_data/filesets/val_utt.txt > ./slp_lab2_data/filesets/val_utter.txt \\n\\n#Create new \\'*_utter.txt\\' files containing only the 4th field of \\'*_utt.txt\\' files accordingly\\ncut -f 4 ./slp_lab2_data/filesets/train_utt.txt > ./slp_lab2_data/filesets/sample_train_utter.txt \\ncut -f 4 ./slp_lab2_data/filesets/test_utt.txt > ./slp_lab2_data/filesets/sample_test_utter.txt \\ncut -f 4 ./slp_lab2_data/filesets/val_utt.txt > ./slp_lab2_data/filesets/sample_val_utter.txt \\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</Users/Savas/opt/anaconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\n#Replace \"_\" with spaces and save the output to \\'*_utt.txt\\' files \\ncat ./slp_lab2_data/filesets/train_utterances.txt |tr \"_\" [:space:] > ./slp_lab2_data/filesets/train_utt.txt \\ncat ./slp_lab2_data/filesets/test_utterances.txt |tr \"_\" [:space:] > ./slp_lab2_data/filesets/test_utt.txt\\ncat ./slp_lab2_data/filesets/validation_utterances.txt |tr \"_\" [:space:] > ./slp_lab2_data/filesets/val_utt.txt\\n\\n#From \\'lexicon.txt\\' we save the first and second field in seperate files\\ncut -f 1 ./slp_lab2_data/lexicon.txt > ./slp_lab2_data/indices_lexicon.txt\\ncut -f 2 ./slp_lab2_data/lexicon.txt > ./slp_lab2_data/temp.txt\\n#From the file containing the second field of \\'lexicon.txt\\' we save every byte from the second one till the last one \\ncut -b 2- ./slp_lab2_data/temp.txt > ./slp_lab2_data/values_lexicon.txt\\n\\n#Create new \\'*_utter.txt\\' files containing only the 3rd field of \\'*_utt.txt\\' files accordingly\\ncut -f 3 ./slp_lab2_data/filesets/train_utt.txt > ./slp_lab2_data/filesets/train_utter.txt \\ncut -f 3 ./slp_lab2_data/filesets/test_utt.txt > ./slp_lab2_data/filesets/test_utter.txt \\ncut -f 3 ./slp_lab2_data/filesets/val_utt.txt > ./slp_lab2_data/filesets/val_utter.txt \\n\\n#Create new \\'*_utter.txt\\' files containing only the 4th field of \\'*_utt.txt\\' files accordingly\\ncut -f 4 ./slp_lab2_data/filesets/train_utt.txt > ./slp_lab2_data/filesets/sample_train_utter.txt \\ncut -f 4 ./slp_lab2_data/filesets/test_utt.txt > ./slp_lab2_data/filesets/sample_test_utter.txt \\ncut -f 4 ./slp_lab2_data/filesets/val_utt.txt > ./slp_lab2_data/filesets/sample_val_utter.txt \\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "#Replace \"_\" with spaces and save the output to '*_utt.txt' files \n",
    "cat ./slp_lab2_data/filesets/train_utterances.txt |tr \"_\" [:space:] > ./slp_lab2_data/filesets/train_utt.txt \n",
    "cat ./slp_lab2_data/filesets/test_utterances.txt |tr \"_\" [:space:] > ./slp_lab2_data/filesets/test_utt.txt\n",
    "cat ./slp_lab2_data/filesets/validation_utterances.txt |tr \"_\" [:space:] > ./slp_lab2_data/filesets/val_utt.txt\n",
    "\n",
    "#From 'lexicon.txt' we save the first and second field in seperate files\n",
    "cut -f 1 ./slp_lab2_data/lexicon.txt > ./slp_lab2_data/indices_lexicon.txt\n",
    "cut -f 2 ./slp_lab2_data/lexicon.txt > ./slp_lab2_data/temp.txt\n",
    "#From the file containing the second field of 'lexicon.txt' we save every byte from the second one till the last one \n",
    "cut -b 2- ./slp_lab2_data/temp.txt > ./slp_lab2_data/values_lexicon.txt\n",
    "\n",
    "#Create new '*_utter.txt' files containing only the 3rd field of '*_utt.txt' files accordingly\n",
    "cut -f 3 ./slp_lab2_data/filesets/train_utt.txt > ./slp_lab2_data/filesets/train_utter.txt \n",
    "cut -f 3 ./slp_lab2_data/filesets/test_utt.txt > ./slp_lab2_data/filesets/test_utter.txt \n",
    "cut -f 3 ./slp_lab2_data/filesets/val_utt.txt > ./slp_lab2_data/filesets/val_utter.txt \n",
    "\n",
    "#Create new '*_utter.txt' files containing only the 4th field of '*_utt.txt' files accordingly\n",
    "cut -f 4 ./slp_lab2_data/filesets/train_utt.txt > ./slp_lab2_data/filesets/sample_train_utter.txt \n",
    "cut -f 4 ./slp_lab2_data/filesets/test_utt.txt > ./slp_lab2_data/filesets/sample_test_utter.txt \n",
    "cut -f 4 ./slp_lab2_data/filesets/val_utt.txt > ./slp_lab2_data/filesets/sample_val_utter.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για την κατασκευή των αρχείων 'uttids' , 'utt2spk' , 'wav.scp' , 'text', ορίζουμε την συνάρτηση 'create_files' ,η οποία δέχεται σαν ορίσμα ενα path προς το αντίστοιχο directory στο οποίο θα αποθηκευθούν τα αρχεία αυτά. Παρακάτω, ακολουθεί σύντομη περιγραφή του τρόπου δημιουργίας κάθε ενός από τα παραπάνω αρχεία.\n",
    "\n",
    "-uttids  : με την εντολή 'open(path+\"/uttids\",\"w+\")', δημιουργούμε στον φάκελο που υπαγορεύεται από την μεταβλητή 'path' ένα κενό αρχείο με όνομα 'uttids'. Συνεχίζοντας, για κάθε ένα από τα τρία σύνολα ανοίγουμε , με δικαιώματα ανάγνωσης, το κατάλληλο αρχείο *_utterances.txt , όπου * = {train, test, dev} , το διαβάζουμε και το αποθηκεύουμε σε μια λίστα και τέλος, \"γράφουμε την λίστα αυτή στο αρχείο uttids. Με αυτό τον τρόπο, επιτυγχάνουμε την αντιγραφή του περιεχόμενου του αρχείου *_utterances.txt στο αρχείο uttids, για κάθε ένα από τα τρία συνόλο δεδομένων. \n",
    "\n",
    "-utt2spk : Με την εντολή 'open(path+\"/utt2spk\",\"w+\")' , δημιουργούμε στον φάκελο που υπαγορεύεται από την μεταβλητή 'path' ένα κενό αρχείο με όνομα 'utt2spk'. Συνεχίζοντας, διαβάζουμε το αρχείο *_utter.txt , που δημιρουργήσαμε παραπάνω,αποθηκεύοντας κάθε γραμμή του αρχείου ως στοιχείο της λίστας glines. Επιπλέον, χρησιμοποιούμε ένα μετρητή , οι τιμές του οποίου κυμένονται από το 1 εώς και το πλήθος των σειρών του κάθε αρχείου. Τέλος, για κάθε στοιχείο της λίστας γράφουμε στο αρχείο utt2spk \"utternace_id_cnt glines[i]\" , όπου cnt η τιμή του μετρητή (που αντιστοιχεί στην γραμμή που επεξεργαζόμαστε) και glines[i] το περιεχόμενο της συγκεκριμένης γραμμής.\n",
    "\n",
    "-wav.scp : Με την εντολή 'open(path+\"/utt2spk\",\"w+\")' , δημιουργούμε στον φάκελο που υπαγορεύεται από την μεταβλητή 'path' ένα κενό αρχείο με όνομα 'wav.scp'. Ακολουθούμε στην συνέχεια την ίδια διαδικασία με παραπάνω , μόνο που αυτή την φορά στο αρχείο 'wav.scp' αντί για glines[i] γράφουμε το path προς το αντίστοιχο αρχείου ήχου. \n",
    "\n",
    "-text : Με την εντολή 'open(path+\"/utt2spk\",\"w+\")' , δημιουργούμε στον φάκελο που υπαγορεύεται από την μεταβλητή 'path' ένα κενό αρχείο με όνομα 'text'. Η συμπλήρωση του αρχείου text απαιτεί αρκετές ενέργειες παραπάνω. Αρχικά, διαβάζουμε κάθε γραμμή του αρχείου transcription.txt και την αποθηκευούμε στην λίστα dlines. Επαναλαμβάνουμε την ίδια διαδικάσια για τα αρχεία indices_lexicon.txt και values_lexicon.txt, αποθηκεύοντας τις γραμμές των αρχείων αυτών στις λίστες keys και values αντίστοιχα. Κάθε στοιχείο της λίστας dlines αποτελεί και μια από τις προτάσεις που λένε οι 4 ομιλητές. Στην συνέχεια, για κάθε στοιχείο της dlines αντικαθιστούμε τους κεφαλαίους χαρακτήρες με τους αντίστοιχους \"μικρόυς\" τους. Επιπλέον, αντικαθηστούμε κάθε ειδικό χαρακτήρα εκτός του \" ' \" με τον χαρακτήρα του κενού και τέλος χωρίζουμε κάθε πρόταση σε λέξεις, αποθηκευόντας το αποτέλεσμα ξανά στο αντίστοιχο στοιχείο της dlines. Μετα την ολοκλήρωση της παραπάνω διαδικασίας, η dlines είναι μια λίστα που περιέχει λίστες και η κάθε \"υπολίστα\" περιέχει τα tokens κάθε πρότασης. Στην συνέχεια, ορίζουμε ενα dictionary \"lexicon\" τα keys του οποίου είναι το περιεχόμενο της λίστας keys και τα values του το αντίστοιχο στοιχείο της λίστας values. Το dictionary αυτό, πρόκειται ουσιαστικά για μία αντιστοιχία κάθε λέξης σε μια ακολουθία φωνημάτων που \"περιγράφουν\" την λέξη αυτή. Στην συνέχεια, με την βοήθεια του dictionary , που δημιουργήσαμε, και της τροποποιημένης dlines ορίζουμε την λίστα pho , της οποίας κάθε στοιχείο είναι η ακολουθία των φωνημάτων που \"περιγράφουν\" την πρόταση που είναι αποθηκεύμενη στην λίστα dlines. Επιπλέον, στην αρχή και στο τέλος κάθε string της λίστας pho προσθέτουμε το φώνημα sil που αντιστοιχεί στην σιωπή. Τέλος, γράφουμε στο αρχείο text \"utterance_id_cnt pho[i]\" , όπου cnt είναι ο ίδιος με παραπάνω μετρητής και pho[i] το i-οστο στοιχείο της λίστας pho.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files(path) :\n",
    "    \n",
    "    f1 = open(path+\"/uttids\",\"w+\")\n",
    "    f2 = open(path+\"/utt2spk\",\"w+\")\n",
    "    f3 = open(path+\"/wav.scp\",\"w+\")\n",
    "    f4 = open(path+\"/text\",\"w+\")\n",
    "    f5 = open(\"/Users/Savas/downloads/slp_lab2_data/indices_lexicon.txt\",\"r\")\n",
    "    f6 = open(\"/Users/Savas/downloads/slp_lab2_data/values_lexicon.txt\",\"r\")\n",
    "    d = open(\"/Users/Savas/downloads/slp_lab2_data/transcription.txt\",\"r\")\n",
    "    \n",
    "    dlines = d.readlines()\n",
    "    keys = f5.readlines()\n",
    "    values = f6.readlines()\n",
    "    \n",
    "    d.close()\n",
    "    f5.close()\n",
    "    f6.close()\n",
    "    \n",
    "    for i in range(0,len(dlines)) :\n",
    "        dlines[i] = dlines[i].lower()\n",
    "        dlines[i] = re.sub('[^a-zA-Z0-9\\'\\s]',' ',dlines[i])\n",
    "        dlines[i] = dlines[i].strip() # not sure if needed \n",
    "        dlines[i]=dlines[i].split()\n",
    "    \n",
    "    for i in range(0,len(keys)):\n",
    "        keys[i] = keys[i].lower()\n",
    "        keys[i] = keys[i].strip('\\n')\n",
    "        \n",
    "    lexicon = {}\n",
    "    for i in range (0,len(keys)):\n",
    "        lexicon[keys[i]] = values[i]\n",
    "    \n",
    "    pho = [\"sil \"]*len(dlines)\n",
    "    for i in range(0,len(dlines)):\n",
    "        for word in dlines[i]:\n",
    "            pho[i] += lexicon[word]\n",
    "            \n",
    "    for i in range(0,len(pho)):\n",
    "        pho[i] = pho[i].replace('\\n',' ')\n",
    "        pho[i] += 'sil' \n",
    "        \n",
    "    if path == train_path:\n",
    "        f = open(\"/Users/Savas/downloads/slp_lab2_data/filesets/train_utterances.txt\",\"r\")\n",
    "        g = open(\"/Users/Savas/downloads/slp_lab2_data/filesets/train_utter.txt\",\"r\")\n",
    "        n = open(\"/Users/Savas/downloads/slp_lab2_data/filesets/sample_train_utter.txt\",\"r\")\n",
    "        \n",
    "        flines = f.readlines()\n",
    "        glines = g.readlines()\n",
    "        nlines = n.readlines()\n",
    "           \n",
    "        f.close()\n",
    "        g.close()\n",
    "        n.close()\n",
    "        d.close()\n",
    "        \n",
    "        cnt= 1\n",
    "        for i in range(0,len(flines)) :\n",
    "            f1.write(flines[i])\n",
    "            f2.write(\"utterance_id_\"+format(cnt,'04d')+\" \"+glines[i])\n",
    "            f3.write(\"utterance_id_\"+format(cnt,'04d')+\" \"+\"/Users/Savas/downloads/slp_lab2_data/wav/\"+glines[i][0:len(glines[i])-1]+\"/\"+flines[i][0:len(flines[i])-1]+\".wav\"+\"\\n\")\n",
    "            f4.write(\"utterance_id_\"+format(cnt,'04d')+\" \"+pho[int(nlines[i][0:len(nlines[i])-1])-1]+'\\n')\n",
    "            cnt+=1\n",
    "            \n",
    "    elif path == test_path :\n",
    "        f = open(\"/Users/Savas/downloads/slp_lab2_data/filesets/test_utterances.txt\",\"r\")\n",
    "        g = open(\"/Users/Savas/downloads/slp_lab2_data/filesets/test_utter.txt\",\"r\")\n",
    "        n = open(\"/Users/Savas/downloads/slp_lab2_data/filesets/sample_test_utter.txt\",\"r\")\n",
    "        \n",
    "        flines = f.readlines()\n",
    "        glines = g.readlines()\n",
    "        nlines = n.readlines()\n",
    "        \n",
    "        f.close()\n",
    "        g.close()\n",
    "        n.close()\n",
    "        \n",
    "        cnt=1\n",
    "        for i in range(0,len(flines)) :\n",
    "            f1.write(flines[i])\n",
    "            f2.write(\"utterance_id_\"+format(cnt,'03d')+\" \"+glines[i])\n",
    "            f3.write(\"utterance_id_\"+format(cnt,'03d')+\" \"+\"/Users/Savas/downloads/slp_lab2_data/wav/\"+glines[i][0:len(glines[i])-1]+\"/\"+flines[i][0:len(flines[i])-1]+\".wav\"+\"\\n\")\n",
    "            f4.write(\"utterance_id_\"+format(cnt,'03d')+\" \"+pho[int(nlines[i][0:len(nlines[i])-1])-1]+'\\n')\n",
    "            cnt+=1   \n",
    "            \n",
    "    elif path == dev_path :\n",
    "        f = open(\"/Users/Savas/downloads/slp_lab2_data/filesets/validation_utterances.txt\",\"r\")\n",
    "        g = open(\"/Users/Savas/downloads/slp_lab2_data/filesets/val_utter.txt\",\"r\")\n",
    "        n = open(\"/Users/Savas/downloads/slp_lab2_data/filesets/sample_val_utter.txt\",\"r\")\n",
    "        \n",
    "        flines = f.readlines()\n",
    "        glines = g.readlines()\n",
    "        nlines = n.readlines()\n",
    "        \n",
    "        f.close()\n",
    "        g.close()\n",
    "        n.close()\n",
    "        \n",
    "        cnt=1\n",
    "        for i in range(0,len(flines)) :\n",
    "            f1.write(flines[i])\n",
    "            f2.write(\"utterance_id_\"+format(cnt,'03d')+\" \"+glines[i])\n",
    "            f3.write(\"utterance_id_\"+format(cnt,'03d')+\" \"+\"/Users/Savas/downloads/slp_lab2_data/wav/\"+glines[i][0:len(glines[i])-1]+\"/\"+flines[i][0:len(flines[i])-1]+\".wav\"+\"\\n\")\n",
    "            f4.write(\"utterance_id_\"+format(cnt,'03d')+\" \"+pho[int(nlines[i][0:len(nlines[i])-1])-1]+'\\n')\n",
    "            cnt+=1\n",
    "    \n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    f3.close()\n",
    "    f4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in paths :\n",
    "    create_files(paths[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Βήματα Κυρίως Μέρους\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Προετοιμασία διαδικασίας αναγνώρισης φωνής για τη USC-TIMIT\n",
    "\n",
    "1. Με τις εντολές \"cp -p /users/savas/kaldi/egs/wsj/s5/path.sh /users/savas/kaldi/egs/usc\" και \"cp -p /users/savas/kaldi/egs/wsj/s5/cmd.sh /users/savas/kaldi/egs/usc\" αντιγράφουμε τα αρχεία \"path.sh\" και \"cmd.sh\" αντίστοιχα, από τον φάκελο wsj/s5 στον φάκελο εργασίας μας egs/usc. Στην συνέχεια , χωρίς την χρήση εντολών bash scripting, τροποποιούμε την τιμή της μεταβλητής KALDI_ROOT , στο path.sh ώστε να ταυτίζεται με το path προς το κέντρικό φάκελο που είναι αποθηκεύμενο το Kaldi. Επιπλέον, αντικαθηστούμε την τιμή των μεταβλητών train_cmd , decode_cmd , cuda_cmd που βρίσκονται στο cmd.sh από queue.pl σε run.pl\n",
    "\n",
    "2. Με την εντολή \"ln -s source destination\" δημιουργούμε soft links εντός του φακέλου usc,όπου και εργαζόμαστε, που δείχνουν τόσο προς τον φάκελο steps που βρίσκεται στο directory kaldi/egs/wsj/s5/steps όσο και προς τον φάκελο utils που βρίσκεται στο directory kaldi/egs/wsj/s5/utils\n",
    "\n",
    "3. Επιπλέον, κάνουμε χρήση της εντολής \"mkdir /users/savas/kaldi/egs/usc/local\" για την δημιουργία του φακέλου local ,εντός του usc, μέσα στον οποίο με την εντολή \"ln -s source destination\" δημιουργούμε ένα ακόμα soft link το οποίο δείχνει αυτή την φορά προς το directory kaldi/egs/wsj/s5/steps/score_kaldi.sh . Το αρχείο score_kaldi.sh θα χρησιμοποιηθεί στην συνέχεια για τον υπολογισμό του PER (Phone Error Rate) \n",
    "\n",
    "4. Στην συνέχεια με την \"mkdir /users/savas/kaldi/egs/usc/conf\" δημιουργούμε τον υπο-φάκελο local μέσα στον οποίο αντιγράφουμε το αρχείο mfcc.conf ,με την εντολή \"cp -p /users/savas/downloads/lab2_help_scripts/mfcc.conf /users/savas/kaldi/egs/usc/conf\", το οποίο δίνεται στις διευκρινίσεις. Tο αρχείο αυτό θα χρησιμοποιηθεί για τον υπολογισμό των MFCC features, με συχνότητα δειγματοληψίας f = 22050Hz. Γενικά η εντολή steps/makefcc.sh, αναζητά για το αρχείο conf/mfcc.conf στον ”conf” για οποιαδήποτε μη default παράμετρο, οι οποίες προωθούνται ως ορίσματα στα αντίστοιχα binary αρχεία.\n",
    "\n",
    "5. Τέλος, με τις εντολές \"mkdir /users/savas/kaldi/egs/usc/data/lang\" , \"mkdir /users/savas/kaldi/egs/usc/data/local\" , δημιουργούμε αρχικά τους υπο-φακέλους lang και local ,αντίστοιχα, εντός του φακέλου data. Στην συνέχεια, κάνουμε χρήση των εντολών \"mkdir /users/savas/kaldi/egs/usc/data/local/dict\" , \"mkdir /users/savas/kaldi/egs/usc/data/local/lm_tmp\" , \"mkdir /users/savas/kaldi/egs/usc/data/local/nist_lm\" για την δημιουργία των φακέλων dict , lm_tmp , nist_lm μέσα στον φάκελο local που δημιουργήσαμε παραπάνω. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ln: /users/savas/kaldi/egs/usc1/steps: File exists\n",
      "ln: /users/savas/kaldi/egs/usc1/utils: File exists\n",
      "mkdir: /users/savas/kaldi/egs/usc1/local: File exists\n",
      "ln: /users/savas/kaldi/egs/usc1/local/score_kaldi.sh: File exists\n",
      "mkdir: /users/savas/kaldi/egs/usc1/conf: File exists\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "#4.1.1\n",
    "cp -p /users/savas/kaldi/egs/wsj/s5/path.sh /users/savas/kaldi/egs/usc\n",
    "cp -p /users/savas/kaldi/egs/wsj/s5/cmd.sh /users/savas/kaldi/egs/usc\n",
    "#Manually change the value of $KALDI_ROOT to path/to/kaldi_directory\n",
    "#Also we set the variables train_cmd, decode_cmd, cuda_cmd to \"run.pl\"\n",
    "\n",
    "#4.1.2 \n",
    "#Creation of Softlinks \n",
    "\n",
    "ln -s /users/savas/kaldi/egs/wsj/s5/steps /users/savas/kaldi/egs/usc\n",
    "ln -s /users/savas/kaldi/egs/wsj/s5/utils /users/savas/kaldi/egs/usc\n",
    "\n",
    "#4.1.3\n",
    "#Creation of folder \"local\", which contains a soft link to score_kaldi.sh\n",
    "\n",
    "mkdir /users/savas/kaldi/egs/usc/local\n",
    "ln -s /users/savas/kaldi/egs/wsj/s5/steps/score_kaldi.sh /users/savas/kaldi/egs/usc/local\n",
    "\n",
    "#4.1.4\n",
    "#Creation of folder \"conf\" containig the modified mfcc.conf file \n",
    "\n",
    "mkdir /users/savas/kaldi/egs/usc/conf\n",
    "cp -p /users/savas/downloads/lab2_help_scripts/mfcc.conf /users/savas/kaldi/egs/usc/conf\n",
    "\n",
    "#4.1.5 \n",
    "#Creation of some extra folders \n",
    "\n",
    "mkdir /users/savas/kaldi/egs/usc/data/lang\n",
    "mkdir /users/savas/kaldi/egs/usc/data/local\n",
    "mkdir /users/savas/kaldi/egs/usc/data/local/dict\n",
    "mkdir /users/savas/kaldi/egs/usc/data/local/lm_tmp\n",
    "mkdir /users/savas/kaldi/egs/usc/data/local/nist_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Προετοιμασία γλωσσικού μοντέλου\n",
    "\n",
    "#### 1. \n",
    "Δημιουργούμε αρχικά , τα αρχεία 'silenece_phones.txt' και 'optional_silence.txt' . Το αρχείο 'optional_silence.txt' περιέχει μόνο το φώνημα sil , ενώ το αρχείο 'silenece_phones.txt'  περιέχει το αρχείο sil καθώς και ένα βοηθητικό φώνημα spn το οποίο στην συνέχεια , κατά την δημιουργία του αρχείο lexicon.txt , θα αντιστοιχιστεί στο σύμβολο < unk >  . Το σύμβολο αυτό θα χρήσιμοποιηθεί για την περιγραφή κάθε oov φωνήματος κατα την δημιουργία του γλωσσικού μας μοντέλου . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of 'silenece_phones.txt'and 'optional_silence.txt' within data/local/dict folder\n",
    "f = open(global_path+'/local/dict/silence_phones.txt','w+')\n",
    "f.write('sil\\n'+ 'spn\\n')\n",
    "f.close()\n",
    "f = open(global_path+'/local/dict/optional_silence.txt','w+')\n",
    "f.write('sil\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στην συνέχεια, στο directory data/local/dict δημιουργούμε το αρχείο lexicon.txt ,το οποίο δεν έχει καμία σχέση με αυτό που παρέχεται στα δεδομένα προς επεξεργασία. Αρχικά, διαβάζουμε και αποθηκεύουμε κάθε γραμμή του αρχείο \"values_lexicon.txt\" σε μια λίστα που ονομάζουμε 'values' . Επιπλέον, ορίζουμε δύο νέες λίστες την temp που θα χρησιμοποιηθεί ως βοηθητική και την phones η οποία τελικά θα είναι αυτή που θα γραφεί στο αρχείο lexicon.txt . Στην συνέχεια, καλούμε την εντολή split σε  κάθε πρόταση ,που είναι αποθηκευμένη ως στοιχείο της λίστας values, παράγοντας έτσι για κάθε πρόταση μια λίστα των λέξεων από τις οποίες αποτελείται. Κάθε τέτοια λίστα που παράγεται την εισάγουμε στην λίστα temp με την εντολή append.  Έπειτα , επαναληπτικά , εξετάζουμε κάθε στοιχείο των υπολιστών της λίστας temp , αν το φώνημα υπό εξέταση βρίσκεται ήδη στην λίστα phones τότε το απορρίπτουμε , διαφορετικά το εισάγουμε σε αυτήν. Από την λίστα phones, η οποία περιέχει από μια φορά κάθε φώνημα που εμφανίζεται στα σύνολο των δεδομένων μας, κρατάμε μόνο αυτά τα οποία δεν περιέχονται στο αρχέιο silence_phones.txt και τα αποθηκεύουμε στην λίστα non_silence_phones, την οποία και ταξινομούμε. Έπειτα, γράφουμε την λίστα non_silence_phones στο αρχείο nonsilence_phones.txt. Στην λίστα phones  εισάγουμε και το φώνημα spn και την ταξινομούμε . Τέλος, το αρχείο lexicon.txt γράφεται ως εξής phone phone , όπου phone είναι το κάθε φώνημα που περιέχεται στην λίστα phones. Σημείωνουμε, πως εισάγουμε τον ενδεχόμενο το phone να είναι το spn όπου σε αυτή την περίπτωση στο αρχείο lexicon.txt  θα γραφεί \" spn   < unk> \" , το οποίο σημαίνει πως κάθε άγνωστη λέξη θα αντιστοιχίζεται μέσω του φωνήματος spn στο σύμβολο  < unk> . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ey', 'f', 'ao', 'r', 't', 'uw', 'w', 'ah', 'n', 'ih', 'p', 'l', 'aa', 'b', 'er', 'g', 'k', 's', 'eh', 'th', 'm', 'd', 'v', 'z', 'iy', 'ae', 'ow', 'ng', 'sh', 'hh', 'aw', 'ay', 'jh', 'y', 'ch', 'zh', 'uh', 'dh', 'oy', 'oov>', '<oov>', 'sil']\n"
     ]
    }
   ],
   "source": [
    "#Creation of 'lexicon.txt' within data/local/dict folder\n",
    "f6 = open(\"/Users/Savas/Downloads/slp_lab2_data/values_lexicon.txt\",\"r\")\n",
    "values = f6.readlines()\n",
    "f6.close()\n",
    "phones = []\n",
    "temp =[]\n",
    "for sentence in values :\n",
    "    temp.append(sentence.split())\n",
    "for i in temp :\n",
    "    for item in i  :\n",
    "        if item not in phones:\n",
    "            phones.append(item)\n",
    "print(phones)\n",
    "non_silence_phones = phones[0:len(phones)-3]\n",
    "non_silence_phones.sort()\n",
    "\n",
    "f = open(global_path+'/local/dict/nonsilence_phones.txt','w+')\n",
    "g = open(global_path+'/local/dict/lexicon.txt','w+')\n",
    "for phone in non_silence_phones :\n",
    "    f.write(phone+'\\n')\n",
    "phones = non_silence_phones\n",
    "phones.append('sil')\n",
    "phones.sort()\n",
    "phones.insert(0,'spn')\n",
    "for phone in phones :\n",
    "    if phone == 'spn' :\n",
    "        g.write('<unk>'+' '+phone+'\\n')\n",
    "    else:\n",
    "        g.write(phone+' '+phone+'\\n') \n",
    "f.close()\n",
    "g.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Συνεχίζοντας, δημιουργούμε τα αρχεία 'lm_train.text', 'lm_test.text' , 'lm_dev.text' στον φάκελο data/local/dict. Συγκεκριμένα , διαβάζουμε και αποθηκεύουμε κάθε γραμμή του αρχείου text ,που δημιουργήσαμε παραπάνω, στην λίστα to_transform. Παράλληλα, ορίζουμε την λίστα final με μήκος όσο και αυτό της λίστας to_transform. Στην συνέχεια, για κάθε στοιχείο της λίστας to_transform αντικάθιστούμε τον χαρακτήρα \"\\n\" με τον χαρακτήρα του κενού και έπειτα αντιγράφουμε το to_transform[i] στοχείο στο final[i] μέχρι να εντοπίσουμε το φώνημα 'sil' . Μόλις εντοπίσουμε το συγκεκριμένο φώνημα εισάγουμε τον χαρακτήρα < s > . Μετά την εισαγωγή του  < s > συνεχίζουμε την αντιγραφή του to_transform[i] ,από εκεί που είχαμε μείνει, στο final[i]. Αφού ολοκληρώσουμε την αντιγραφή των φωνημάτων από το to_transform[i], εισάγουμε στο final[i] το χαρακτήρα < /s >. Έτσι, επιτυγχάνουμε την εισαγωγή των χαρακτήρων < s > και < /s > , τα οποία σηματοδοτούν την αρχή και το τέλος μια πρότασης ,αντίστοιχα, σε κάθε ακολουθία φωνημάτων.  Τέλος, δημιουργούμε και το κενό αρχείο extra_questions.txt στο φάκελο data/local/dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of 'lm_train.text', 'lm_test.text' 'lm_dev.text' within data/local/dict folder\n",
    "for i in paths:\n",
    "    f = open(global_path+'/local/dict/lm_{}.text'.format(i),'w+')\n",
    "    g = open(paths[i]+'/text','r')\n",
    "    to_transform = g.readlines()\n",
    "    final = ['']*len(to_transform)\n",
    "    for j in range(0,len(to_transform)) :\n",
    "        to_transform[j] = to_transform[j].replace('\\n',' ')\n",
    "        final[j]+=to_transform[j][0:to_transform[j].index('sil')]+'<s> '+to_transform[j][to_transform[j].index('sil'):]+'</s>\\n' \n",
    "    for stuff in final :\n",
    "        f.write(stuff)\n",
    "    f.close()\n",
    "    g.close()\n",
    "#Creation of 'extra_questions.txt' within data/local/dict folder\n",
    "f = open(global_path+'/local/dict/extra_questions.txt',\"w+\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - 3. \n",
    "• Γίνεται χρήση της εντολής build-lm.sh του πακέτου IRSTLM. Το script κανει split την διαδικασία της         εκτίμησης σε 1 και 2 jobs αντίστοιχα για τους φακέλους test, train και dev αντίστοιχα, με σκοπό την\n",
    "δημιουργία των unigram και bigram μοντέλων αντίστοιχα\n",
    "\n",
    "• Το script παράγει ένα αρχείο LM στην μορφή .ilm.gz που δεν είναι η τελική ARPA μορφή, αλλά\n",
    "μία ενδιάμεση που ονομάζεται iARPA και η οποία αναγνωρίζεται και απαιτείται από την compile-lm.sh\n",
    ",καθώς και από τον αποκωδικοποιητή Moses SMT που τρέχει με το πακέτο IRSTLM.\n",
    "\n",
    "Κατασκευάζουμε τόσο unigram , όσο και bigram γλωσσικά μοντέλα για ta trainnig dataset ,dev dataset , καθώς και test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGFILE:/dev/null\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$scr/build-sublm.pl $verbose $prune $prune_thr_str $smoothing \"$additional_smoothing_parameters\" --size $order --ngrams \"$gunzip -c $tmpdir/ngram.${sdict}.gz\" -sublm $tmpdir/lm.$sdict $additional_parameters >> $logfile 2>&1\n",
      "LOGFILE:/dev/null\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$scr/build-sublm.pl $verbose $prune $prune_thr_str $smoothing \"$additional_smoothing_parameters\" --size $order --ngrams \"$gunzip -c $tmpdir/ngram.${sdict}.gz\" -sublm $tmpdir/lm.$sdict $additional_parameters >> $logfile 2>&1\n",
      "LOGFILE:/dev/null\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$scr/build-sublm.pl $verbose $prune $prune_thr_str $smoothing \"$additional_smoothing_parameters\" --size $order --ngrams \"$gunzip -c $tmpdir/ngram.${sdict}.gz\" -sublm $tmpdir/lm.$sdict $additional_parameters >> $logfile 2>&1\n",
      "LOGFILE:/dev/null\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$scr/build-sublm.pl $verbose $prune $prune_thr_str $smoothing \"$additional_smoothing_parameters\" --size $order --ngrams \"$gunzip -c $tmpdir/ngram.${sdict}.gz\" -sublm $tmpdir/lm.$sdict $additional_parameters >> $logfile 2>&1\n",
      "LOGFILE:/dev/null\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$scr/build-sublm.pl $verbose $prune $prune_thr_str $smoothing \"$additional_smoothing_parameters\" --size $order --ngrams \"$gunzip -c $tmpdir/ngram.${sdict}.gz\" -sublm $tmpdir/lm.$sdict $additional_parameters >> $logfile 2>&1\n",
      "LOGFILE:/dev/null\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$scr/build-sublm.pl $verbose $prune $prune_thr_str $smoothing \"$additional_smoothing_parameters\" --size $order --ngrams \"$gunzip -c $tmpdir/ngram.${sdict}.gz\" -sublm $tmpdir/lm.$sdict $additional_parameters >> $logfile 2>&1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inpfile: /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_train_uni.ilm.gz\n",
      "outfile: /dev/stdout\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 1510\n",
      "OOV code is 1510\n",
      "Saving in txt format to /dev/stdout\n",
      "inpfile: /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_train_bi.ilm.gz\n",
      "outfile: /dev/stdout\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 1510\n",
      "OOV code is 1510\n",
      "Saving in txt format to /dev/stdout\n",
      "inpfile: /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_test_uni.ilm.gz\n",
      "outfile: /dev/stdout\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 226\n",
      "OOV code is 226\n",
      "Saving in txt format to /dev/stdout\n",
      "inpfile: /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_test_bi.ilm.gz\n",
      "outfile: /dev/stdout\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 226\n",
      "OOV code is 226\n",
      "Saving in txt format to /dev/stdout\n",
      "inpfile: /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_dev_uni.ilm.gz\n",
      "outfile: /dev/stdout\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 225\n",
      "OOV code is 225\n",
      "Saving in txt format to /dev/stdout\n",
      "inpfile: /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_dev_bi.ilm.gz\n",
      "outfile: /dev/stdout\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 225\n",
      "OOV code is 225\n",
      "Saving in txt format to /dev/stdout\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "source /users/savas/kaldi/egs/usc/path.sh\n",
    "export IRSTLM=/Users/Savas/kaldi/tools/irstlm\n",
    "\n",
    "#Build an intermediate form of unigram and bigram language model for the trainning set \n",
    "/Users/Savas/kaldi/tools/irstlm/bin/build-lm.sh -i /Users/Savas/kaldi/egs/usc/data/local/dict/lm_train.text -n 1 -o /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_train_uni.ilm.gz -k 5\n",
    "/Users/Savas/kaldi/tools/irstlm/bin/build-lm.sh -i /Users/Savas/kaldi/egs/usc/data/local/dict/lm_train.text -n 2 -o /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_train_bi.ilm.gz -k 5\n",
    "\n",
    "#Compile the intermediate form of both unigram and bigram LMs of the trainning set to their final forms \n",
    "compile-lm /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_train_uni.ilm.gz -t=yes /dev/stdout | grep -v unk | gzip -c > /Users/Savas/kaldi/egs/usc/data/local/nist_lm/lm_train_uni.arpa.gz\n",
    "compile-lm /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_train_bi.ilm.gz -t=yes /dev/stdout | grep -v unk | gzip -c > /Users/Savas/kaldi/egs/usc/data/local/nist_lm/lm_train_bi.arpa.gz\n",
    "\n",
    "#Build an intermediate form of unigram and bigram language model for the test set \n",
    "/Users/Savas/kaldi/tools/irstlm/bin/build-lm.sh -i /Users/Savas/kaldi/egs/usc/data/local/dict/lm_test.text -n 1 -o /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_test_uni.ilm.gz -k 5\n",
    "/Users/Savas/kaldi/tools/irstlm/bin/build-lm.sh -i /Users/Savas/kaldi/egs/usc/data/local/dict/lm_test.text -n 2 -o /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_test_bi.ilm.gz -k 5\n",
    "\n",
    "#Compile the intermediate form of both unigram and bigram LMs of the test set to their final forms \n",
    "compile-lm /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_test_uni.ilm.gz -t=yes /dev/stdout | grep -v unk | gzip -c > /Users/Savas/kaldi/egs/usc/data/local/nist_lm/lm_test_uni.arpa.gz\n",
    "compile-lm /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_test_bi.ilm.gz -t=yes /dev/stdout | grep -v unk | gzip -c > /Users/Savas/kaldi/egs/usc/data/local/nist_lm/lm_test_bi.arpa.gz\n",
    "\n",
    "#Build an intermediate form of unigram and bigram language model for the validation set \n",
    "/Users/Savas/kaldi/tools/irstlm/bin/build-lm.sh -i /Users/Savas/kaldi/egs/usc/data/local/dict/lm_dev.text -n 1 -o /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_dev_uni.ilm.gz -k 5\n",
    "/Users/Savas/kaldi/tools/irstlm/bin/build-lm.sh -i /Users/Savas/kaldi/egs/usc/data/local/dict/lm_dev.text -n 2 -o /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_dev_bi.ilm.gz -k 5\n",
    "\n",
    "#Compile the intermediate form of both unigram and bigram LMs of the validation set to their final forms \n",
    "compile-lm /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_dev_uni.ilm.gz -t=yes /dev/stdout | grep -v unk | gzip -c > /Users/Savas/kaldi/egs/usc/data/local/nist_lm/lm_dev_uni.arpa.gz\n",
    "compile-lm /Users/Savas/kaldi/egs/usc/data/local/lm_tmp/lm_dev_bi.ilm.gz -t=yes /dev/stdout | grep -v unk | gzip -c > /Users/Savas/kaldi/egs/usc/data/local/nist_lm/lm_dev_bi.arpa.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. \n",
    "Γίνεται εκτέλεση της εντολής prepare lang.sh με ορίσματα LM arpa αρχεία πλέον και των 6 μοντέλων για παραγωγή των αρχείων ”words.txt”,”oov.txt και ”phones.txt”, που περιέχουν αντίστοιχα:\n",
    "• Μία λίστα από όλες τις λέξεις στο vocabulary, επιπρόσθετα του sil και #0 συμβόλων (χρήση ως μετάβαση ε στο input του G.fst). Κάθε λέξη έχει μοναδικό index\n",
    "• Μία λίστα από όλα τα φωνήματα στο vocabulary,\n",
    "•  ́Εχει μία μόνο γραμμή με την λέξη (όχι το φώνημα) για τα εκτός vocabulary αντικείμενα. Εδώ χρησιμοποιείται το \"oov\" γιατί αυτό παίρνουμε από το πακέτο IRSTLM στα γλωσσσικά μοντέλα μας.\n",
    "\n",
    "Ακόμα δημιουργούνται δύο fst. Το πρώτο είναι το ”L.fst”, ένας πεπερασμένων καταστάσεων μετατροπέας από το λεξικό, με σύμβολα-φωνημάτα στην είσοδο και σύμβολα-λέξεις στην έξοδο. Το L κάνει mapping monophone ακολουθίες σε λέξεις. Το δεύτερο είναι το ”L_disambig.fst”, το φωνητικό λεξικό για αποσαφήνιση των ασαφειών ,διφορούμενων συμβόλων-φωνημάτων, τα οποία τα χρειαζόμαστε όταν έχουμε μία λέξη που είναι prefix μιας άλλης (πχ. cat και cats στο ίδιο λεξικό). Εάν δεν τα έχεις, τότε τα μοντέλα γίνονται μη ντετερμινιστικά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utils/prepare_lang.sh /users/savas/kaldi/egs/usc/data/local/dict <unk> /users/savas/kaldi/egs/usc/data/local/tmp /users/savas/kaldi/egs/usc/data/lang\n",
      "Checking /users/savas/kaldi/egs/usc/data/local/dict/silence_phones.txt ...\n",
      "--> reading /users/savas/kaldi/egs/usc/data/local/dict/silence_phones.txt\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> /users/savas/kaldi/egs/usc/data/local/dict/silence_phones.txt is OK\n",
      "\n",
      "Checking /users/savas/kaldi/egs/usc/data/local/dict/optional_silence.txt ...\n",
      "--> reading /users/savas/kaldi/egs/usc/data/local/dict/optional_silence.txt\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> /users/savas/kaldi/egs/usc/data/local/dict/optional_silence.txt is OK\n",
      "\n",
      "Checking /users/savas/kaldi/egs/usc/data/local/dict/nonsilence_phones.txt ...\n",
      "--> reading /users/savas/kaldi/egs/usc/data/local/dict/nonsilence_phones.txt\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> /users/savas/kaldi/egs/usc/data/local/dict/nonsilence_phones.txt is OK\n",
      "\n",
      "Checking disjoint: silence_phones.txt, nonsilence_phones.txt\n",
      "--> disjoint property is OK.\n",
      "\n",
      "Checking /users/savas/kaldi/egs/usc/data/local/dict/lexicon.txt\n",
      "--> reading /users/savas/kaldi/egs/usc/data/local/dict/lexicon.txt\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> /users/savas/kaldi/egs/usc/data/local/dict/lexicon.txt is OK\n",
      "\n",
      "Checking /users/savas/kaldi/egs/usc/data/local/dict/lexiconp.txt\n",
      "--> reading /users/savas/kaldi/egs/usc/data/local/dict/lexiconp.txt\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> /users/savas/kaldi/egs/usc/data/local/dict/lexiconp.txt is OK\n",
      "\n",
      "Checking lexicon pair /users/savas/kaldi/egs/usc/data/local/dict/lexicon.txt and /users/savas/kaldi/egs/usc/data/local/dict/lexiconp.txt\n",
      "--> lexicon pair /users/savas/kaldi/egs/usc/data/local/dict/lexicon.txt and /users/savas/kaldi/egs/usc/data/local/dict/lexiconp.txt match\n",
      "\n",
      "Checking /users/savas/kaldi/egs/usc/data/local/dict/extra_questions.txt ...\n",
      "--> /users/savas/kaldi/egs/usc/data/local/dict/extra_questions.txt is empty (this is OK)\n",
      "--> SUCCESS [validating dictionary directory /users/savas/kaldi/egs/usc/data/local/dict]\n",
      "\n",
      "prepare_lang.sh: validating output directory\n",
      "utils/validate_lang.pl /users/savas/kaldi/egs/usc/data/lang\n",
      "Checking existence of separator file\n",
      "separator file /users/savas/kaldi/egs/usc/data/lang/subword_separator.txt is empty or does not exist, deal in word case.\n",
      "Checking /users/savas/kaldi/egs/usc/data/lang/phones.txt ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones.txt is OK\n",
      "\n",
      "Checking words.txt: #0 ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/words.txt is OK\n",
      "\n",
      "Checking disjoint: silence.txt, nonsilence.txt, disambig.txt ...\n",
      "--> silence.txt and nonsilence.txt are disjoint\n",
      "--> silence.txt and disambig.txt are disjoint\n",
      "--> disambig.txt and nonsilence.txt are disjoint\n",
      "--> disjoint property is OK\n",
      "\n",
      "Checking sumation: silence.txt, nonsilence.txt, disambig.txt ...\n",
      "--> found no unexplainable phones in phones.txt\n",
      "\n",
      "Checking /users/savas/kaldi/egs/usc/data/lang/phones/context_indep.{txt, int, csl} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 10 entry/entries in /users/savas/kaldi/egs/usc/data/lang/phones/context_indep.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/context_indep.int corresponds to /users/savas/kaldi/egs/usc/data/lang/phones/context_indep.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/context_indep.csl corresponds to /users/savas/kaldi/egs/usc/data/lang/phones/context_indep.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/context_indep.{txt, int, csl} are OK\n",
      "\n",
      "Checking /users/savas/kaldi/egs/usc/data/lang/phones/nonsilence.{txt, int, csl} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 156 entry/entries in /users/savas/kaldi/egs/usc/data/lang/phones/nonsilence.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/nonsilence.int corresponds to /users/savas/kaldi/egs/usc/data/lang/phones/nonsilence.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/nonsilence.csl corresponds to /users/savas/kaldi/egs/usc/data/lang/phones/nonsilence.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/nonsilence.{txt, int, csl} are OK\n",
      "\n",
      "Checking /users/savas/kaldi/egs/usc/data/lang/phones/silence.{txt, int, csl} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 10 entry/entries in /users/savas/kaldi/egs/usc/data/lang/phones/silence.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/silence.int corresponds to /users/savas/kaldi/egs/usc/data/lang/phones/silence.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/silence.csl corresponds to /users/savas/kaldi/egs/usc/data/lang/phones/silence.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/silence.{txt, int, csl} are OK\n",
      "\n",
      "Checking /users/savas/kaldi/egs/usc/data/lang/phones/optional_silence.{txt, int, csl} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 1 entry/entries in /users/savas/kaldi/egs/usc/data/lang/phones/optional_silence.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/optional_silence.int corresponds to /users/savas/kaldi/egs/usc/data/lang/phones/optional_silence.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/optional_silence.csl corresponds to /users/savas/kaldi/egs/usc/data/lang/phones/optional_silence.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/optional_silence.{txt, int, csl} are OK\n",
      "\n",
      "Checking /users/savas/kaldi/egs/usc/data/lang/phones/disambig.{txt, int, csl} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 2 entry/entries in /users/savas/kaldi/egs/usc/data/lang/phones/disambig.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/disambig.int corresponds to /users/savas/kaldi/egs/usc/data/lang/phones/disambig.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/disambig.csl corresponds to /users/savas/kaldi/egs/usc/data/lang/phones/disambig.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/disambig.{txt, int, csl} are OK\n",
      "\n",
      "Checking /users/savas/kaldi/egs/usc/data/lang/phones/roots.{txt, int} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 41 entry/entries in /users/savas/kaldi/egs/usc/data/lang/phones/roots.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/roots.int corresponds to /users/savas/kaldi/egs/usc/data/lang/phones/roots.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/roots.{txt, int} are OK\n",
      "\n",
      "Checking /users/savas/kaldi/egs/usc/data/lang/phones/sets.{txt, int} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 41 entry/entries in /users/savas/kaldi/egs/usc/data/lang/phones/sets.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/sets.int corresponds to /users/savas/kaldi/egs/usc/data/lang/phones/sets.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/sets.{txt, int} are OK\n",
      "\n",
      "Checking /users/savas/kaldi/egs/usc/data/lang/phones/extra_questions.{txt, int} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 9 entry/entries in /users/savas/kaldi/egs/usc/data/lang/phones/extra_questions.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/extra_questions.int corresponds to /users/savas/kaldi/egs/usc/data/lang/phones/extra_questions.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/extra_questions.{txt, int} are OK\n",
      "\n",
      "Checking /users/savas/kaldi/egs/usc/data/lang/phones/word_boundary.{txt, int} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 166 entry/entries in /users/savas/kaldi/egs/usc/data/lang/phones/word_boundary.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/word_boundary.int corresponds to /users/savas/kaldi/egs/usc/data/lang/phones/word_boundary.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/word_boundary.{txt, int} are OK\n",
      "\n",
      "Checking optional_silence.txt ...\n",
      "--> reading /users/savas/kaldi/egs/usc/data/lang/phones/optional_silence.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/optional_silence.txt is OK\n",
      "\n",
      "Checking disambiguation symbols: #0 and #1\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/disambig.txt has \"#0\" and \"#1\"\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/disambig.txt is OK\n",
      "\n",
      "Checking topo ...\n",
      "\n",
      "Checking word_boundary.txt: silence.txt, nonsilence.txt, disambig.txt ...\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/word_boundary.txt doesn't include disambiguation symbols\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/word_boundary.txt is the union of nonsilence.txt and silence.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/word_boundary.txt is OK\n",
      "\n",
      "Checking word-level disambiguation symbols...\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/phones/wdisambig.txt exists (newer prepare_lang.sh)\n",
      "Checking word_boundary.int and disambig.int\n",
      "--> generating a 53 word/subword sequence\n",
      "--> resulting phone sequence from L.fst corresponds to the word sequence\n",
      "--> L.fst is OK\n",
      "--> generating a 12 word/subword sequence\n",
      "--> resulting phone sequence from L_disambig.fst corresponds to the word sequence\n",
      "--> L_disambig.fst is OK\n",
      "\n",
      "Checking /users/savas/kaldi/egs/usc/data/lang/oov.{txt, int} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 1 entry/entries in /users/savas/kaldi/egs/usc/data/lang/oov.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/oov.int corresponds to /users/savas/kaldi/egs/usc/data/lang/oov.txt\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/oov.{txt, int} are OK\n",
      "\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/L.fst is olabel sorted\n",
      "--> /users/savas/kaldi/egs/usc/data/lang/L_disambig.fst is olabel sorted\n",
      "--> SUCCESS [validating lang directory /users/savas/kaldi/egs/usc/data/lang]\n",
      "Preparing train, dev and test data\n",
      "Preparing language models for test\n",
      "Succeeded in formatting data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fstaddselfloops /users/savas/kaldi/egs/usc/data/lang/phones/wdisambig_phones.int /users/savas/kaldi/egs/usc/data/lang/phones/wdisambig_words.int \n",
      "arpa2fst --disambig-symbol=#0 --read-symbol-table=data/lang_test/words.txt - data/lang_test/G_train_uni.fst \n",
      "LOG (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:94) Reading \\data\\ section.\n",
      "LOG (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:149) Reading \\1-grams: section.\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 7 [-4.43838\tutterance_id_0001] skipped: word 'utterance_id_0001' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 22 [-4.43838\tutterance_id_0002] skipped: word 'utterance_id_0002' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 34 [-4.43838\tutterance_id_0003] skipped: word 'utterance_id_0003' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 38 [-4.43838\tutterance_id_0004] skipped: word 'utterance_id_0004' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 41 [-4.43838\tutterance_id_0005] skipped: word 'utterance_id_0005' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 46 [-4.43838\tutterance_id_0006] skipped: word 'utterance_id_0006' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 48 [-4.43838\tutterance_id_0007] skipped: word 'utterance_id_0007' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 50 [-4.43838\tutterance_id_0008] skipped: word 'utterance_id_0008' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 52 [-4.43838\tutterance_id_0009] skipped: word 'utterance_id_0009' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 53 [-4.43838\tutterance_id_0010] skipped: word 'utterance_id_0010' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 54 [-4.43838\tutterance_id_0011] skipped: word 'utterance_id_0011' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 55 [-4.43838\tutterance_id_0012] skipped: word 'utterance_id_0012' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 56 [-4.43838\tutterance_id_0013] skipped: word 'utterance_id_0013' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 57 [-4.43838\tutterance_id_0014] skipped: word 'utterance_id_0014' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 58 [-4.43838\tutterance_id_0015] skipped: word 'utterance_id_0015' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 60 [-4.43838\tutterance_id_0016] skipped: word 'utterance_id_0016' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 61 [-4.43838\tutterance_id_0017] skipped: word 'utterance_id_0017' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 62 [-4.43838\tutterance_id_0018] skipped: word 'utterance_id_0018' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 63 [-4.43838\tutterance_id_0019] skipped: word 'utterance_id_0019' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 64 [-4.43838\tutterance_id_0020] skipped: word 'utterance_id_0020' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 66 [-4.43838\tutterance_id_0021] skipped: word 'utterance_id_0021' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 67 [-4.43838\tutterance_id_0022] skipped: word 'utterance_id_0022' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 68 [-4.43838\tutterance_id_0023] skipped: word 'utterance_id_0023' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 70 [-4.43838\tutterance_id_0024] skipped: word 'utterance_id_0024' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 71 [-4.43838\tutterance_id_0025] skipped: word 'utterance_id_0025' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 72 [-4.43838\tutterance_id_0026] skipped: word 'utterance_id_0026' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 73 [-4.43838\tutterance_id_0027] skipped: word 'utterance_id_0027' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 74 [-4.43838\tutterance_id_0028] skipped: word 'utterance_id_0028' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 75 [-4.43838\tutterance_id_0029] skipped: word 'utterance_id_0029' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 76 [-4.43838\tutterance_id_0030] skipped: word 'utterance_id_0030' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:259) Of 1468 parse warnings, 30 were reported. Run program with --max_warnings=-1 to see all warnings\n",
      "LOG (arpa2fst[5.5.585~1-7b762]:RemoveRedundantStates():arpa-lm-compiler.cc:359) Reduced num-states from 1 to 1\n",
      "arpa2fst --disambig-symbol=#0 --read-symbol-table=data/lang_test/words.txt - data/lang_test/G_dev_uni.fst \n",
      "LOG (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:94) Reading \\data\\ section.\n",
      "LOG (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:149) Reading \\1-grams: section.\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 7 [-3.54357\tutterance_id_001] skipped: word 'utterance_id_001' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 25 [-3.54357\tutterance_id_002] skipped: word 'utterance_id_002' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 35 [-3.54357\tutterance_id_003] skipped: word 'utterance_id_003' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 42 [-3.54357\tutterance_id_004] skipped: word 'utterance_id_004' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 45 [-3.54357\tutterance_id_005] skipped: word 'utterance_id_005' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 47 [-3.54357\tutterance_id_006] skipped: word 'utterance_id_006' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 49 [-3.54357\tutterance_id_007] skipped: word 'utterance_id_007' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 53 [-3.54357\tutterance_id_008] skipped: word 'utterance_id_008' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 54 [-3.54357\tutterance_id_009] skipped: word 'utterance_id_009' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 55 [-3.54357\tutterance_id_010] skipped: word 'utterance_id_010' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 56 [-3.54357\tutterance_id_011] skipped: word 'utterance_id_011' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 57 [-3.54357\tutterance_id_012] skipped: word 'utterance_id_012' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 58 [-3.54357\tutterance_id_013] skipped: word 'utterance_id_013' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 60 [-3.54357\tutterance_id_014] skipped: word 'utterance_id_014' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 61 [-3.54357\tutterance_id_015] skipped: word 'utterance_id_015' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 62 [-3.54357\tutterance_id_016] skipped: word 'utterance_id_016' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 63 [-3.54357\tutterance_id_017] skipped: word 'utterance_id_017' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 64 [-3.54357\tutterance_id_018] skipped: word 'utterance_id_018' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 65 [-3.54357\tutterance_id_019] skipped: word 'utterance_id_019' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 66 [-3.54357\tutterance_id_020] skipped: word 'utterance_id_020' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 69 [-3.54357\tutterance_id_021] skipped: word 'utterance_id_021' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 70 [-3.54357\tutterance_id_022] skipped: word 'utterance_id_022' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 71 [-3.54357\tutterance_id_023] skipped: word 'utterance_id_023' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 72 [-3.54357\tutterance_id_024] skipped: word 'utterance_id_024' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 73 [-3.54357\tutterance_id_025] skipped: word 'utterance_id_025' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 74 [-3.54357\tutterance_id_026] skipped: word 'utterance_id_026' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 75 [-3.54357\tutterance_id_027] skipped: word 'utterance_id_027' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 76 [-3.54357\tutterance_id_028] skipped: word 'utterance_id_028' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 77 [-3.54357\tutterance_id_029] skipped: word 'utterance_id_029' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 78 [-3.54357\tutterance_id_030] skipped: word 'utterance_id_030' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:259) Of 183 parse warnings, 30 were reported. Run program with --max_warnings=-1 to see all warnings\n",
      "LOG (arpa2fst[5.5.585~1-7b762]:RemoveRedundantStates():arpa-lm-compiler.cc:359) Reduced num-states from 1 to 1\n",
      "arpa2fst --disambig-symbol=#0 --read-symbol-table=data/lang_test/words.txt - data/lang_test/G_test_uni.fst \n",
      "LOG (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:94) Reading \\data\\ section.\n",
      "LOG (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:149) Reading \\1-grams: section.\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 7 [-3.53256\tutterance_id_001] skipped: word 'utterance_id_001' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 27 [-3.53256\tutterance_id_002] skipped: word 'utterance_id_002' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 37 [-3.53256\tutterance_id_003] skipped: word 'utterance_id_003' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 45 [-3.53256\tutterance_id_004] skipped: word 'utterance_id_004' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 48 [-3.53256\tutterance_id_005] skipped: word 'utterance_id_005' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 49 [-3.53256\tutterance_id_006] skipped: word 'utterance_id_006' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 51 [-3.53256\tutterance_id_007] skipped: word 'utterance_id_007' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 53 [-3.53256\tutterance_id_008] skipped: word 'utterance_id_008' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 54 [-3.53256\tutterance_id_009] skipped: word 'utterance_id_009' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 55 [-3.53256\tutterance_id_010] skipped: word 'utterance_id_010' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 56 [-3.53256\tutterance_id_011] skipped: word 'utterance_id_011' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 57 [-3.53256\tutterance_id_012] skipped: word 'utterance_id_012' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 58 [-3.53256\tutterance_id_013] skipped: word 'utterance_id_013' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 59 [-3.53256\tutterance_id_014] skipped: word 'utterance_id_014' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 60 [-3.53256\tutterance_id_015] skipped: word 'utterance_id_015' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 62 [-3.53256\tutterance_id_016] skipped: word 'utterance_id_016' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 64 [-3.53256\tutterance_id_017] skipped: word 'utterance_id_017' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 65 [-3.53256\tutterance_id_018] skipped: word 'utterance_id_018' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 66 [-3.53256\tutterance_id_019] skipped: word 'utterance_id_019' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 68 [-3.53256\tutterance_id_020] skipped: word 'utterance_id_020' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 69 [-3.53256\tutterance_id_021] skipped: word 'utterance_id_021' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 70 [-3.53256\tutterance_id_022] skipped: word 'utterance_id_022' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 71 [-3.53256\tutterance_id_023] skipped: word 'utterance_id_023' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 72 [-3.53256\tutterance_id_024] skipped: word 'utterance_id_024' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 73 [-3.53256\tutterance_id_025] skipped: word 'utterance_id_025' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 74 [-3.53256\tutterance_id_026] skipped: word 'utterance_id_026' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 75 [-3.53256\tutterance_id_027] skipped: word 'utterance_id_027' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 76 [-3.53256\tutterance_id_028] skipped: word 'utterance_id_028' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 77 [-3.53256\tutterance_id_029] skipped: word 'utterance_id_029' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 78 [-3.53256\tutterance_id_030] skipped: word 'utterance_id_030' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:259) Of 184 parse warnings, 30 were reported. Run program with --max_warnings=-1 to see all warnings\n",
      "LOG (arpa2fst[5.5.585~1-7b762]:RemoveRedundantStates():arpa-lm-compiler.cc:359) Reduced num-states from 1 to 1\n",
      "arpa2fst --disambig-symbol=#0 --read-symbol-table=data/lang_test/words.txt - data/lang_test/G_train_bi.fst \n",
      "LOG (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:94) Reading \\data\\ section.\n",
      "LOG (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:149) Reading \\1-grams: section.\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 9 [-4.43838\tutterance_id_0001\t-0.30103] skipped: word 'utterance_id_0001' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 23 [-4.43838\tutterance_id_0002\t-0.30103] skipped: word 'utterance_id_0002' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 35 [-4.43838\tutterance_id_0003\t-0.30103] skipped: word 'utterance_id_0003' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 39 [-4.43838\tutterance_id_0004\t-0.30103] skipped: word 'utterance_id_0004' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 42 [-4.43838\tutterance_id_0005\t-0.30103] skipped: word 'utterance_id_0005' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 47 [-4.43838\tutterance_id_0006\t-0.30103] skipped: word 'utterance_id_0006' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 49 [-4.43838\tutterance_id_0007\t-0.30103] skipped: word 'utterance_id_0007' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 51 [-4.43838\tutterance_id_0008\t-0.30103] skipped: word 'utterance_id_0008' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 53 [-4.43838\tutterance_id_0009\t-0.30103] skipped: word 'utterance_id_0009' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 54 [-4.43838\tutterance_id_0010\t-0.30103] skipped: word 'utterance_id_0010' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 55 [-4.43838\tutterance_id_0011\t-0.30103] skipped: word 'utterance_id_0011' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 56 [-4.43838\tutterance_id_0012\t-0.30103] skipped: word 'utterance_id_0012' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 57 [-4.43838\tutterance_id_0013\t-0.30103] skipped: word 'utterance_id_0013' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 58 [-4.43838\tutterance_id_0014\t-0.30103] skipped: word 'utterance_id_0014' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 59 [-4.43838\tutterance_id_0015\t-0.30103] skipped: word 'utterance_id_0015' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 61 [-4.43838\tutterance_id_0016\t-0.30103] skipped: word 'utterance_id_0016' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 62 [-4.43838\tutterance_id_0017\t-0.30103] skipped: word 'utterance_id_0017' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 63 [-4.43838\tutterance_id_0018\t-0.30103] skipped: word 'utterance_id_0018' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 64 [-4.43838\tutterance_id_0019\t-0.30103] skipped: word 'utterance_id_0019' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 65 [-4.43838\tutterance_id_0020\t-0.30103] skipped: word 'utterance_id_0020' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 67 [-4.43838\tutterance_id_0021\t-0.30103] skipped: word 'utterance_id_0021' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 68 [-4.43838\tutterance_id_0022\t-0.30103] skipped: word 'utterance_id_0022' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 69 [-4.43838\tutterance_id_0023\t-0.30103] skipped: word 'utterance_id_0023' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 71 [-4.43838\tutterance_id_0024\t-0.30103] skipped: word 'utterance_id_0024' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 72 [-4.43838\tutterance_id_0025\t-0.30103] skipped: word 'utterance_id_0025' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 73 [-4.43838\tutterance_id_0026\t-0.30103] skipped: word 'utterance_id_0026' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 74 [-4.43838\tutterance_id_0027\t-0.30103] skipped: word 'utterance_id_0027' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 75 [-4.43838\tutterance_id_0028\t-0.30103] skipped: word 'utterance_id_0028' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 76 [-4.43838\tutterance_id_0029\t-0.30103] skipped: word 'utterance_id_0029' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 77 [-4.43838\tutterance_id_0030\t-0.30103] skipped: word 'utterance_id_0030' not in symbol table\n",
      "LOG (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:149) Reading \\2-grams: section.\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:259) Of 2938 parse warnings, 30 were reported. Run program with --max_warnings=-1 to see all warnings\n",
      "LOG (arpa2fst[5.5.585~1-7b762]:RemoveRedundantStates():arpa-lm-compiler.cc:359) Reduced num-states from 42 to 42\n",
      "arpa2fst --disambig-symbol=#0 --read-symbol-table=data/lang_test/words.txt - data/lang_test/G_dev_bi.fst \n",
      "LOG (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:94) Reading \\data\\ section.\n",
      "LOG (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:149) Reading \\1-grams: section.\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 9 [-3.54357\tutterance_id_001\t-0.30103] skipped: word 'utterance_id_001' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 26 [-3.54357\tutterance_id_002\t-0.30103] skipped: word 'utterance_id_002' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 36 [-3.54357\tutterance_id_003\t-0.30103] skipped: word 'utterance_id_003' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 43 [-3.54357\tutterance_id_004\t-0.30103] skipped: word 'utterance_id_004' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 46 [-3.54357\tutterance_id_005\t-0.30103] skipped: word 'utterance_id_005' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 48 [-3.54357\tutterance_id_006\t-0.30103] skipped: word 'utterance_id_006' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 50 [-3.54357\tutterance_id_007\t-0.30103] skipped: word 'utterance_id_007' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 54 [-3.54357\tutterance_id_008\t-0.30103] skipped: word 'utterance_id_008' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 55 [-3.54357\tutterance_id_009\t-0.30103] skipped: word 'utterance_id_009' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 56 [-3.54357\tutterance_id_010\t-0.30103] skipped: word 'utterance_id_010' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 57 [-3.54357\tutterance_id_011\t-0.30103] skipped: word 'utterance_id_011' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 58 [-3.54357\tutterance_id_012\t-0.30103] skipped: word 'utterance_id_012' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 59 [-3.54357\tutterance_id_013\t-0.30103] skipped: word 'utterance_id_013' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 61 [-3.54357\tutterance_id_014\t-0.30103] skipped: word 'utterance_id_014' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 62 [-3.54357\tutterance_id_015\t-0.30103] skipped: word 'utterance_id_015' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 63 [-3.54357\tutterance_id_016\t-0.30103] skipped: word 'utterance_id_016' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 64 [-3.54357\tutterance_id_017\t-0.30103] skipped: word 'utterance_id_017' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 65 [-3.54357\tutterance_id_018\t-0.30103] skipped: word 'utterance_id_018' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 66 [-3.54357\tutterance_id_019\t-0.30103] skipped: word 'utterance_id_019' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 67 [-3.54357\tutterance_id_020\t-0.30103] skipped: word 'utterance_id_020' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 70 [-3.54357\tutterance_id_021\t-0.30103] skipped: word 'utterance_id_021' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 71 [-3.54357\tutterance_id_022\t-0.30103] skipped: word 'utterance_id_022' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 72 [-3.54357\tutterance_id_023\t-0.30103] skipped: word 'utterance_id_023' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 73 [-3.54357\tutterance_id_024\t-0.30103] skipped: word 'utterance_id_024' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 74 [-3.54357\tutterance_id_025\t-0.30103] skipped: word 'utterance_id_025' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 75 [-3.54357\tutterance_id_026\t-0.30103] skipped: word 'utterance_id_026' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 76 [-3.54357\tutterance_id_027\t-0.30103] skipped: word 'utterance_id_027' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 77 [-3.54357\tutterance_id_028\t-0.30103] skipped: word 'utterance_id_028' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 78 [-3.54357\tutterance_id_029\t-0.30103] skipped: word 'utterance_id_029' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 79 [-3.54357\tutterance_id_030\t-0.30103] skipped: word 'utterance_id_030' not in symbol table\n",
      "LOG (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:149) Reading \\2-grams: section.\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:259) Of 368 parse warnings, 30 were reported. Run program with --max_warnings=-1 to see all warnings\n",
      "LOG (arpa2fst[5.5.585~1-7b762]:RemoveRedundantStates():arpa-lm-compiler.cc:359) Reduced num-states from 42 to 42\n",
      "arpa2fst --disambig-symbol=#0 --read-symbol-table=data/lang_test/words.txt - data/lang_test/G_test_bi.fst \n",
      "LOG (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:94) Reading \\data\\ section.\n",
      "LOG (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:149) Reading \\1-grams: section.\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 9 [-3.53256\tutterance_id_001\t-0.30103] skipped: word 'utterance_id_001' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 28 [-3.53256\tutterance_id_002\t-0.30103] skipped: word 'utterance_id_002' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 38 [-3.53256\tutterance_id_003\t-0.30103] skipped: word 'utterance_id_003' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 46 [-3.53256\tutterance_id_004\t-0.30103] skipped: word 'utterance_id_004' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 49 [-3.53256\tutterance_id_005\t-0.30103] skipped: word 'utterance_id_005' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 50 [-3.53256\tutterance_id_006\t-0.30103] skipped: word 'utterance_id_006' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 52 [-3.53256\tutterance_id_007\t-0.30103] skipped: word 'utterance_id_007' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 54 [-3.53256\tutterance_id_008\t-0.30103] skipped: word 'utterance_id_008' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 55 [-3.53256\tutterance_id_009\t-0.30103] skipped: word 'utterance_id_009' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 56 [-3.53256\tutterance_id_010\t-0.30103] skipped: word 'utterance_id_010' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 57 [-3.53256\tutterance_id_011\t-0.30103] skipped: word 'utterance_id_011' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 58 [-3.53256\tutterance_id_012\t-0.30103] skipped: word 'utterance_id_012' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 59 [-3.53256\tutterance_id_013\t-0.30103] skipped: word 'utterance_id_013' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 60 [-3.53256\tutterance_id_014\t-0.30103] skipped: word 'utterance_id_014' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 61 [-3.53256\tutterance_id_015\t-0.30103] skipped: word 'utterance_id_015' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 63 [-3.53256\tutterance_id_016\t-0.30103] skipped: word 'utterance_id_016' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 65 [-3.53256\tutterance_id_017\t-0.30103] skipped: word 'utterance_id_017' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 66 [-3.53256\tutterance_id_018\t-0.30103] skipped: word 'utterance_id_018' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 67 [-3.53256\tutterance_id_019\t-0.30103] skipped: word 'utterance_id_019' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 69 [-3.53256\tutterance_id_020\t-0.30103] skipped: word 'utterance_id_020' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 70 [-3.53256\tutterance_id_021\t-0.30103] skipped: word 'utterance_id_021' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 71 [-3.53256\tutterance_id_022\t-0.30103] skipped: word 'utterance_id_022' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 72 [-3.53256\tutterance_id_023\t-0.30103] skipped: word 'utterance_id_023' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 73 [-3.53256\tutterance_id_024\t-0.30103] skipped: word 'utterance_id_024' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 74 [-3.53256\tutterance_id_025\t-0.30103] skipped: word 'utterance_id_025' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 75 [-3.53256\tutterance_id_026\t-0.30103] skipped: word 'utterance_id_026' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 76 [-3.53256\tutterance_id_027\t-0.30103] skipped: word 'utterance_id_027' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 77 [-3.53256\tutterance_id_028\t-0.30103] skipped: word 'utterance_id_028' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 78 [-3.53256\tutterance_id_029\t-0.30103] skipped: word 'utterance_id_029' not in symbol table\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:219) line 79 [-3.53256\tutterance_id_030\t-0.30103] skipped: word 'utterance_id_030' not in symbol table\n",
      "LOG (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:149) Reading \\2-grams: section.\n",
      "WARNING (arpa2fst[5.5.585~1-7b762]:Read():arpa-file-parser.cc:259) Of 370 parse warnings, 30 were reported. Run program with --max_warnings=-1 to see all warnings\n",
      "LOG (arpa2fst[5.5.585~1-7b762]:RemoveRedundantStates():arpa-lm-compiler.cc:359) Reduced num-states from 42 to 42\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "source /users/savas/kaldi/egs/usc/path.sh\n",
    "cd /users/savas/kaldi/egs/usc\n",
    "#We construct our language's lexicon, represented by an fst (L.fst)\n",
    "utils/prepare_lang.sh  /users/savas/kaldi/egs/usc/data/local/dict  '<unk>'  /users/savas/kaldi/egs/usc/data/local/tmp  /users/savas/kaldi/egs/usc/data/lang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τα G.fst, μπορούν να δομηθούν αποκλειστικά πάνω στα unigram και bigram μοντέλα μας. Για την διαδικασσία χρησιμοποιείται η τροποποιημένη timit_format_data.sh του kaldi , το οποίο το ονομάζουμε timit_format_data1.sh . Πιο συγκεκριμένα, δημιουργούμε το αντίγραφο ”lang_test_i_j”, όπου i = {train,dev,test} και j = {uni,bi}, που περιέχει τα αρχεία και τα αυτόματα του προηγούμενου βήματος. Mέσα σε κάθε φάκελο που δημιουργεί το script ,καλούνται οι εντολές gunzip -c και arpa2fst, επαναληπτικά και για 6 γλωσσικά μοντέλα, με σκοπό την δημιουργία των αντίστοιχων γραμμτικών σε μορφή \"G.fst\". Η arpa2fst μάλιστα δέχεται όρισμα την λίστα ”words.txt”, και λογικό αφού θέλουμε πληροφορία για όλες τις δυνατές λέξεις και φωνημάτα που έχουν προκύψει από όλες τις προτάσεις (άρα για αυτό έγινε εξέταση όλων train,test,dev, αφού συνολικά καλύπτουν 100% της πληροφορίας)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "source /users/savas/kaldi/egs/usc/path.sh\n",
    "cd /users/savas/kaldi/egs/usc\n",
    "\n",
    "#We construct our language's grammar, represented by an fst (G.fst) for both unigram and bigram LMs\n",
    "./timit_format_data1.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Επιπλέον, δημιουργούμε από κάθε αρχείο utt2spk το αντίστοιχο spk2utt με την κλήση του κώδικα pearl  utt2spk_to_spk2utt.pl, το οποίο θα χρειαστεί παρακάτω. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /users/savas/kaldi/egs/usc/path.sh\n",
    "cd /users/savas/kaldi/egs/usc\n",
    "\n",
    "#Create the spk2utt file from the utt2spk one for all the datasets \n",
    "utils/utt2spk_to_spk2utt.pl data/train/utt2spk > data/train/spk2utt\n",
    "utils/utt2spk_to_spk2utt.pl data/test/utt2spk > data/test/spk2utt\n",
    "utils/utt2spk_to_spk2utt.pl data/dev/utt2spk > data/dev/spk2utt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ερώτημα 1\n",
    "\n",
    "• Η αξιολόγηση γλωσσικών μοντέλων περιλαμβάνει και τον υπολογισμό perplexity, εντροπίας και out-of-\n",
    "vocabulary rate τόσο του test set όσο και του validation set.\n",
    "\n",
    "• Μεγάλα LM αρχεία μπορούν να \"περιοριστούν\" με έξυπνο τρόπο με την εντολή που αναφέραμε και νωρίτερα, prune, η οποία αφαιρεί n-gram μοντέλα τα οποία για την προσφυγή σε back-off αποτελέσματα οδηγούν σε μικρές-μηδαμινές απώλειες. Η εντολή αυτή δέχεται ως όρισμα ένα threshold,το οποίο έχει νόημα γα τα bigram μοντέλα του test και του validation set και το οποίο βέβαια εχει αξία για εμπειρικά δεδομένα. (threshold = 0 δεν οδηγεί σε περιορισμό).\n",
    "\n",
    "• Η εφαμοργή του pruning οδηγεί σε LM αρχεία μορφής ”.plm” που περιέχει λιγότερα bigrams, για τις προδιαγραφές του παρόντος εργαστηρίου. Το output είναι ένα ARPA LM αρχείο. Με σκοπό να μετρήσουμε την απώλεια της ακρίβειας που εισήχθη με το pruning, το perplexity του output LM μπορεί να υπολογιστεί (δεν θα διαφέρει πολύ από το αντίστοιχο LM χωρίς εφαρμογή pruning).\n",
    "\n",
    "• Τα LM είναι όντως χρήσιμα και έχουν αξία όταν έχουμε αρκετά δεδομένα στην διάθεση μας, και έτσι μπορούμε να απευθύνουμε queries σε αυτά για υπολογισμό peplexity που έχει ουσία. Συγκεκριμένα, οι προδιαγραφές του εργαστήριου προβλέπουν αρκετά δεδομένα, και για αυτόν ακριβώς τον λόγο ΠΡΕΠΕΙ να παρατηρηθεί ότι το perplexity των bigram μοντέλων είναι χαμηλότερο από το αντίστοιχο των unigram μοντέλων τόσο για το test set όσο και για το validation set.\n",
    "\n",
    "• Υπολογίζεται με την εντολή compile-lm, αλλά αυτή την φορά με όρισμα –eval=test. Παρακάτω αναδεικνύον- ται τα αποτελέσματα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################\n",
      "Calculating perplexity for evaluation data unigram model\n",
      "########################################################\n",
      "%% Nw=6540 PP=40.13 PPwp=0.00 Nbo=0 Noov=0 OOV=0.00%\n",
      "\n",
      "#######################################################\n",
      "Calculating perplexity for evaluation data bigram model\n",
      "#######################################################\n",
      "%% Nw=6540 PP=18.24 PPwp=0.00 Nbo=182 Noov=0 OOV=0.00%\n",
      "\n",
      "##################################################\n",
      "Calculating perplexity for test data unigram model\n",
      "##################################################\n",
      "%% Nw=6363 PP=39.75 PPwp=0.00 Nbo=0 Noov=0 OOV=0.00%\n",
      "\n",
      "#################################################\n",
      "Calculating perplexity for test data bigram model\n",
      "#################################################\n",
      "%% Nw=6363 PP=17.61 PPwp=0.00 Nbo=183 Noov=0 OOV=0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOV code is 225\n",
      "OOV code is 225\n",
      "pruning LM with thresholds: \n",
      "\n",
      "DEBUG_LEVEL:0/1 Everything OK\n",
      "inpfile: lm_dev_uni.plm\n",
      "outfile: lm_dev_uni.plm.blm\n",
      "evalfile: ../dict/lm_dev.text\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 225\n",
      "OOV code is 225\n",
      "Start Eval\n",
      "OOV code: 225\n",
      "OOV code is 225\n",
      "OOV code is 225\n",
      "pruning LM with thresholds: \n",
      " 1e-06\n",
      "DEBUG_LEVEL:0/1 Everything OK\n",
      "inpfile: lm_dev_bi.plm\n",
      "outfile: lm_dev_bi.plm.blm\n",
      "evalfile: ../dict/lm_dev.text\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 225\n",
      "OOV code is 225\n",
      "Start Eval\n",
      "OOV code: 225\n",
      "OOV code is 226\n",
      "OOV code is 226\n",
      "pruning LM with thresholds: \n",
      "\n",
      "DEBUG_LEVEL:0/1 Everything OK\n",
      "inpfile: lm_test_uni.plm\n",
      "outfile: lm_test_uni.plm.blm\n",
      "evalfile: ../dict/lm_test.text\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 226\n",
      "OOV code is 226\n",
      "Start Eval\n",
      "OOV code: 226\n",
      "OOV code is 226\n",
      "OOV code is 226\n",
      "pruning LM with thresholds: \n",
      " 1e-06\n",
      "DEBUG_LEVEL:0/1 Everything OK\n",
      "inpfile: lm_test_bi.plm\n",
      "outfile: lm_test_bi.plm.blm\n",
      "evalfile: ../dict/lm_test.text\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 226\n",
      "OOV code is 226\n",
      "Start Eval\n",
      "OOV code: 226\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "source /users/savas/kaldi/egs/usc/path.sh\n",
    "cd /users/savas/kaldi/egs/usc/data/local/lm_tmp\n",
    "export IRSTLM=$KALDI_ROOT/tools/irstlm/\n",
    "export PATH=${PATH}:$IRSTLM/bin\n",
    "\n",
    "\n",
    "echo \"########################################################\"\n",
    "echo \"Calculating perplexity for evaluation data unigram model\"\n",
    "echo \"########################################################\"\n",
    "\n",
    "prune-lm --threshold=1e-6,1e-6 lm_dev_uni.ilm.gz lm_dev_uni.plm\n",
    "compile-lm lm_dev_uni.plm --eval=../dict/lm_dev.text --dub=10000000\n",
    "echo\"\"\n",
    "echo \"#######################################################\"\n",
    "echo \"Calculating perplexity for evaluation data bigram model\"\n",
    "echo \"#######################################################\"\n",
    "prune-lm --threshold=1e-6,1e-6 lm_dev_bi.ilm.gz lm_dev_bi.plm\n",
    "compile-lm lm_dev_bi.plm --eval=../dict/lm_dev.text --dub=10000000\n",
    "echo\"\"\n",
    "echo \"##################################################\"\n",
    "echo \"Calculating perplexity for test data unigram model\"\n",
    "echo \"##################################################\"\n",
    "\n",
    "prune-lm --threshold=1e-6,1e-6 lm_test_uni.ilm.gz lm_test_uni.plm\n",
    "compile-lm lm_test_uni.plm --eval=../dict/lm_test.text --dub=10000000\n",
    "echo\"\"\n",
    "echo \"#################################################\"\n",
    "echo \"Calculating perplexity for test data bigram model\"\n",
    "echo \"#################################################\"\n",
    "prune-lm --threshold=1e-6,1e-6 lm_test_bi.ilm.gz lm_test_bi.plm\n",
    "compile-lm lm_test_bi.plm --eval=../dict/lm_test.text --dub=10000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Επομένως παρατηρείτει πτώση από 40.13 σε 18.24 στο unigram μοντέλο και από 39.75 σε 17.61 στο bigram μοντέλο  για το validation και το test set αντίστοιχα, πράγμα απόλυτα δικαιολογημένο, αφού όσο χαμηλότερο είναι τόσο πιο αποτελεσματική κατανομή πιθανοτήτων έχουμε για πρόβλεψη δειγμάτων."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Εξαγωγή ακουστικών χαρακτηριστικών\n",
    "\n",
    "Η διαδικασσία εξαγωγής features αναφέρεται και ως παραμετροποίηση ομιλίας, δηλαδή η ανάλυση των σημάτων ήχου σε μικρή κλίμακα για στατιστικούς σκοπούς και για κατηγοριοποίηση των φασματικών features των σημάτων με σκοπό την προετοιμασία τους για την μετέπειτα αποκωδικοποίηση. Ουσιαστικά η διαδικασία αυτή ”γεφυρώνει” πληροφορίες ήχου και στατιστικών μοντέλων ASR.\n",
    "\n",
    "Οι δύο πιο γνωστές μορφές είναι MFCC(Mel Frequency Cepstral Coefficients) και PLP (Perceptual Linear Prediction) με την πρώτη μορφή να λαμβάνει υπόψιν την φύση της φωνής ,ενώ με το LPC γίνεται πρόβλεψη μελλοντικών features βάσει προηγούμενων. Εφόσον η ανθρώπινη φωνή και το ακουστικό σύστημα του ανθρώπου είναι μη γραμμικά, το LPC δεν είναι καλή επιλογή για εκτίμηση ακουστικών features. Αντίθετα, τα MFCC βασίζονται στην λογική των λογαριθμικά διαχωρισμένων φίλτρων και για αυτό θεωρούνται αποτελεσματικότερα.\n",
    "\n",
    "Τώρα θα παράγουμε τα features και τα αντίστοιχα ”feats.scp” που κάνει mapping τα utterance ids στις θέσεις ενός αρχείου, εδώ ”feats.ark.”. Για GMM-HMM συστήματα, τυπικά χρησιμοποιούμε MFCC ή PLP features και μετά εφαρμόζουμε cepstral mean και variance normalisation\n",
    "\n",
    "Εδώ θα παράξουμε MFCCs -για τα οποία δίνουμε μια σχετική θεωρητική ανάλυση στην αρχή της παρούσας αναφοράς- για τα δεδομένα μας. Συγκεκριμένα, για να αντισταθμίσουμε την μεταβολή speech και speaker, η εφαρμογή της παραπάνω μεθόδου στα MFCC κρίνεται αναγκαία.\n",
    "\n",
    "Εκτελούμε τις εντολές kaldi make_mfcc.sh και compute_cmvn_stats.sh, για την παραγωγή των όσων αναφέρθηκαν,για κάθε ένα από τα τρία directory ξεχωριστά. .Τα τελικά αποτελέσματα περιέχονται στους φακέλους ”/data/*/data\" , όπου * ={train,dec,test} , σε μορφή ”.ark”, ενώ εκείνα των στατιστικών αναλύσεων στον φάκελο ”/data/*/data/log\",όπου * ={train,dec,test}, σε μορφή ”.log”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps/make_mfcc.sh data/train\n",
      "steps/make_mfcc.sh: moving data/train/feats.scp to data/train/.backup\n",
      "steps/make_mfcc.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.\n",
      "steps/make_mfcc.sh: Succeeded creating MFCC features for train\n",
      "steps/make_mfcc.sh data/test\n",
      "steps/make_mfcc.sh: moving data/test/feats.scp to data/test/.backup\n",
      "steps/make_mfcc.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.\n",
      "steps/make_mfcc.sh: Succeeded creating MFCC features for test\n",
      "steps/make_mfcc.sh data/dev\n",
      "steps/make_mfcc.sh: moving data/dev/feats.scp to data/dev/.backup\n",
      "steps/make_mfcc.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.\n",
      "steps/make_mfcc.sh: Succeeded creating MFCC features for dev\n",
      "steps/compute_cmvn_stats.sh data/train\n",
      "Succeeded creating CMVN stats for train\n",
      "steps/compute_cmvn_stats.sh data/test\n",
      "Succeeded creating CMVN stats for test\n",
      "steps/compute_cmvn_stats.sh data/dev\n",
      "Succeeded creating CMVN stats for dev\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source /users/savas/kaldi/egs/usc/path.sh\n",
    "cd /users/savas/kaldi/egs/usc\n",
    "\n",
    "#Computation of MFFCS \n",
    "steps/make_mfcc.sh data/train\n",
    "steps/make_mfcc.sh data/test\n",
    "steps/make_mfcc.sh data/dev\n",
    "\n",
    "#Compute the Cepstral Means and perform their Variance Normalization\n",
    "steps/compute_cmvn_stats.sh data/train\n",
    "steps/compute_cmvn_stats.sh data/test\n",
    "steps/compute_cmvn_stats.sh data/dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ερώτημα 2\n",
    "\n",
    "Στην αναγνώριση φωνής χρειάζεται να αφαιρέσουμε οποιοδήποτε σήμα περιπλέκεται με το σήμα του ομιλητή καθώς δεν μπορούμε να εξάγουμε καθαρά και έγκυρα τις παραμέτρους . Τέτοια σήματα μπορεί να προέρχονται από το περιβάλλον του ομιλητή ,όπως θόρυβος, άλλες ομιλίες και εν γένη άλλες ηχητικές πηγές. ́Εχοντας το σήμα εισόδου x[n] το οποίο περνάει από φίλτρο απόκρισης h[n] παίρνουμε ένα σήμα εξόδου y[n]=x[n]*h[n] το οποίο είναι η συνέλιξη των x[n] και h[n].Χρησιμοποιώντας την ιδιότητα του Μ/Σ Fourier ισχύει ότι: Y [f] = X[f]H[f]\n",
    "\n",
    "Σαν επόμενο βήμα παίρνουμε cepstrum μέσω του λογαρίθμου του φάσματος:Y[q] = log(X[f]H[F]) = log(X[f]) + log(Y [f]) = X[q] + H[q]\n",
    " \n",
    "Παρατήρηση: Από την συνέλιξη στο πεδίο του χρόνου μεταβήκαμε στον πολλαπλασιασμό στο πεδίο της συχνότητας και στην πρόσθεση στο πεδίο του cepsrtum. Τα παραπάνω βήματα θα χρειαστούν για τον υπολογισμό του Cepstral Mean Normalization.\n",
    " \n",
    "Πλέον με τις μετατροπές αυτές γνωρίζουμε ότι οποιεσδήποτε αλλοιώσεις έχουμε στο πεδίο του χρόνου από ανεπιθύμητους παράγοντες στο πεδίο του cepstrum τους έχουμε σε μορφή αθροίσματος Κάνοντας λοιπόν την υπόθεση ότι όλα αυτά είναι στατικά(stationary) μπορούμε να θωρήσουμε ότι το i-οστό πλαίσιο έχει την μορφή: Yi[q] = H[q] + Xi[q].\n",
    "\n",
    "Παίρνοντας τον μέσο όρο των πλαισίων του συνόλου των πλαισίων έχουμε : N1 􏰀i Yi[q] = H[q]+ N1 􏰀i Xi[q]\n",
    "Τώρα ορίζουμε την διαφορά “Ri”του i-οστού πλαισίου ως αυτήν του μέσου όρου ,που υπολογίσαμε πριν\n",
    " ,από την έξοδο: Ri[q] = Yi[q] − 1 􏰀 Yi[q] = H[q] + Xi[q] − (H[q] + 1 􏰀 Xi[q]). Επομένως: Ri[q] =\n",
    "1􏰀NiNi\n",
    "Xi[q] που εν τέλη καταλήγει να είναι η διαφορά του Xi από τον μέσο όρο όλων των παραθύρων\n",
    "\n",
    "Το Cepstral Mean Normalization δεν είναι υποχρεωτικό ,ειδικά όταν πρόκειται για έναν ομιλητή σε ένα φυσιολογικό περιβάλλον χωρίς εξαιρετικό θόρυβο. Ωστόσο κρίνεται εξαιρετικό εργαλείο για την εξαγωγή καλύτερων καλύτερων ποσοστών με τον συνδυασμό ενός κέρδους(gain) πάνω στο σήμα καθώς τότε τα ποσοστά επιτυχούς αναγνώρισης είναι σημαντικά καλύτερα . Παρατήρηση:  ́Εχει αποδειχθεί ότι η μέθοδος CMVN είναι ιδιαίτερα αποτελεσματική για μικρές εκφράσεις(utterances)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ερώτημα 3\n",
    "\n",
    "Γίνεται εκτέλεση του παρακάτω bash script ,στο οποίο εκτελείται η εντολή feat-to-dim η οποία\n",
    "επιστρέφει την διάσταση των MFCC για το πρώτο από τα 4 sections που έχουμε κάνει split Dimension = 13. Σημειώνεται πως το dimension αυτό θα είναι ίδιο για όλα τα features του set\n",
    "\n",
    "Επιπλέον ,γίνεται εκτέλεση της feat-to-len η οποία επιστρέφει τον αριθμό των frames για κάθε ένα από τα 46 utterrances id’s στο πρώτο από τα 4 section, εμείς θέλουμε τα 5 πρώτα. Number of Features = [371,336,519,400,397] για τα utterance_ids = [4,16,43,48,53]. Η αντιστοιχία από το utterance_id σε αυτό τον πίνακα που γράψαμε παραπάνω γίνεται με την βοήθεια του αρχείο uttids που φτιάξαμε κατα την προπαρασκευή."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feat-to-dim ark:/users/savas/kaldi/egs/usc/data/train/data/raw_mfcc_train.1.ark - \n",
      "feat-to-len scp:/users/savas/kaldi/egs/usc/data/test/feats.scp ark,t:/users/savas/kaldi/egs/usc/data/test/feats.length \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source /users/savas/kaldi/egs/usc/path.sh\n",
    "\n",
    "feat-to-dim ark:/users/savas/kaldi/egs/usc/data/train/data/raw_mfcc_train.1.ark -\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utterance_id_001 371 \n",
      "utterance_id_002 336 \n",
      "utterance_id_003 519 \n",
      "utterance_id_004 400 \n",
      "utterance_id_005 397 \n",
      "utterance_id_006 531 \n",
      "utterance_id_007 327 \n",
      "utterance_id_008 461 \n",
      "utterance_id_009 527 \n",
      "utterance_id_010 515 \n",
      "utterance_id_011 442 \n",
      "utterance_id_012 405 \n",
      "utterance_id_013 446 \n",
      "utterance_id_014 465 \n",
      "utterance_id_015 509 \n",
      "utterance_id_016 379 \n",
      "utterance_id_017 539 \n",
      "utterance_id_018 503 \n",
      "utterance_id_019 338 \n",
      "utterance_id_020 347 \n",
      "utterance_id_021 462 \n",
      "utterance_id_022 376 \n",
      "utterance_id_023 394 \n",
      "utterance_id_024 532 \n",
      "utterance_id_025 668 \n",
      "utterance_id_026 421 \n",
      "utterance_id_027 579 \n",
      "utterance_id_028 343 \n",
      "utterance_id_029 571 \n",
      "utterance_id_030 420 \n",
      "utterance_id_031 494 \n",
      "utterance_id_032 550 \n",
      "utterance_id_033 445 \n",
      "utterance_id_034 361 \n",
      "utterance_id_035 309 \n",
      "utterance_id_036 390 \n",
      "utterance_id_037 417 \n",
      "utterance_id_038 508 \n",
      "utterance_id_039 274 \n",
      "utterance_id_040 348 \n",
      "utterance_id_041 494 \n",
      "utterance_id_042 516 \n",
      "utterance_id_043 377 \n",
      "utterance_id_044 393 \n",
      "utterance_id_045 448 \n",
      "utterance_id_046 432 \n",
      "utterance_id_047 450 \n",
      "utterance_id_048 489 \n",
      "utterance_id_049 302 \n",
      "utterance_id_050 467 \n",
      "utterance_id_051 397 \n",
      "utterance_id_052 417 \n",
      "utterance_id_053 585 \n",
      "utterance_id_054 407 \n",
      "utterance_id_055 435 \n",
      "utterance_id_056 548 \n",
      "utterance_id_057 345 \n",
      "utterance_id_058 350 \n",
      "utterance_id_059 282 \n",
      "utterance_id_060 352 \n",
      "utterance_id_061 449 \n",
      "utterance_id_062 371 \n",
      "utterance_id_063 408 \n",
      "utterance_id_064 446 \n",
      "utterance_id_065 453 \n",
      "utterance_id_066 364 \n",
      "utterance_id_067 482 \n",
      "utterance_id_068 440 \n",
      "utterance_id_069 419 \n",
      "utterance_id_070 319 \n",
      "utterance_id_071 482 \n",
      "utterance_id_072 458 \n",
      "utterance_id_073 700 \n",
      "utterance_id_074 457 \n",
      "utterance_id_075 403 \n",
      "utterance_id_076 344 \n",
      "utterance_id_077 394 \n",
      "utterance_id_078 586 \n",
      "utterance_id_079 462 \n",
      "utterance_id_080 354 \n",
      "utterance_id_081 421 \n",
      "utterance_id_082 495 \n",
      "utterance_id_083 321 \n",
      "utterance_id_084 452 \n",
      "utterance_id_085 411 \n",
      "utterance_id_086 606 \n",
      "utterance_id_087 425 \n",
      "utterance_id_088 503 \n",
      "utterance_id_089 379 \n",
      "utterance_id_090 434 \n",
      "utterance_id_091 354 \n",
      "utterance_id_092 459 \n",
      "utterance_id_093 283 \n",
      "utterance_id_094 453 \n",
      "utterance_id_095 416 \n",
      "utterance_id_096 372 \n",
      "utterance_id_097 367 \n",
      "utterance_id_098 365 \n",
      "utterance_id_099 429 \n",
      "utterance_id_100 474 \n",
      "utterance_id_101 472 \n",
      "utterance_id_102 650 \n",
      "utterance_id_103 528 \n",
      "utterance_id_104 570 \n",
      "utterance_id_105 398 \n",
      "utterance_id_106 361 \n",
      "utterance_id_107 318 \n",
      "utterance_id_108 402 \n",
      "utterance_id_109 398 \n",
      "utterance_id_110 475 \n",
      "utterance_id_111 603 \n",
      "utterance_id_112 355 \n",
      "utterance_id_113 355 \n",
      "utterance_id_114 522 \n",
      "utterance_id_115 423 \n",
      "utterance_id_116 468 \n",
      "utterance_id_117 561 \n",
      "utterance_id_118 454 \n",
      "utterance_id_119 443 \n",
      "utterance_id_120 416 \n",
      "utterance_id_121 414 \n",
      "utterance_id_122 466 \n",
      "utterance_id_123 539 \n",
      "utterance_id_124 556 \n",
      "utterance_id_125 571 \n",
      "utterance_id_126 495 \n",
      "utterance_id_127 444 \n",
      "utterance_id_128 598 \n",
      "utterance_id_129 556 \n",
      "utterance_id_130 508 \n",
      "utterance_id_131 534 \n",
      "utterance_id_132 528 \n",
      "utterance_id_133 561 \n",
      "utterance_id_134 429 \n",
      "utterance_id_135 504 \n",
      "utterance_id_136 395 \n",
      "utterance_id_137 465 \n",
      "utterance_id_138 282 \n",
      "utterance_id_139 442 \n",
      "utterance_id_140 590 \n",
      "utterance_id_141 413 \n",
      "utterance_id_142 378 \n",
      "utterance_id_143 228 \n",
      "utterance_id_144 525 \n",
      "utterance_id_145 404 \n",
      "utterance_id_146 352 \n",
      "utterance_id_147 364 \n",
      "utterance_id_148 291 \n",
      "utterance_id_149 373 \n",
      "utterance_id_150 390 \n",
      "utterance_id_151 520 \n",
      "utterance_id_152 342 \n",
      "utterance_id_153 348 \n",
      "utterance_id_154 422 \n",
      "utterance_id_155 640 \n",
      "utterance_id_156 359 \n",
      "utterance_id_157 446 \n",
      "utterance_id_158 351 \n",
      "utterance_id_159 352 \n",
      "utterance_id_160 379 \n",
      "utterance_id_161 286 \n",
      "utterance_id_162 350 \n",
      "utterance_id_163 471 \n",
      "utterance_id_164 470 \n",
      "utterance_id_165 506 \n",
      "utterance_id_166 274 \n",
      "utterance_id_167 327 \n",
      "utterance_id_168 423 \n",
      "utterance_id_169 417 \n",
      "utterance_id_170 546 \n",
      "utterance_id_171 471 \n",
      "utterance_id_172 462 \n",
      "utterance_id_173 468 \n",
      "utterance_id_174 463 \n",
      "utterance_id_175 421 \n",
      "utterance_id_176 346 \n",
      "utterance_id_177 517 \n",
      "utterance_id_178 472 \n",
      "utterance_id_179 502 \n",
      "utterance_id_180 419 \n",
      "utterance_id_181 429 \n",
      "utterance_id_182 260 \n",
      "utterance_id_183 355 \n",
      "utterance_id_184 514 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feat-to-len scp:/users/savas/kaldi/egs/usc/data/test/feats.scp ark,t:/users/savas/kaldi/egs/usc/data/test/feats.lengths \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source /users/savas/kaldi/egs/usc/path.sh\n",
    "\n",
    "feat-to-len scp:/users/savas/kaldi/egs/usc/data/test/feats.scp ark,t:/users/savas/kaldi/egs/usc/data/test/feats.lengths\n",
    "cat /users/savas/kaldi/egs/usc/data/test/feats.lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Εκπαίδευση ακουστικών μοντέλων και αποκωδικοποίηση προτάσεων\n",
    "\n",
    "Η αποκωδικοποιητική ικανότητα ενός συστήματος ASR εξαρτάται σχεδόν εξ’ολοκλήρου από τις επιδράσεις του γλωσσικού και του ακουστικού μοντέλου που κατασκευάσαμε. Το αντικείμενο του πρώτου είναι ο υπολογισμός της πιθανότητας μιας πρότασης στο τωρινό task αναγνώρισμης ανάμεσα σε ένα μεγάλο αριθμό προτάσεων, πράγμα που είναι κρίσιμο για την μείωση της πολυπλοκότητας της αναζήτησης των λέξεων στον αποκωδικοποιητή που θα κατασκευάσουμε. Επομένως, αναμένουμε το bigram μοντέλο που είναι πιο αποτελεσματικό από το unigram να οδηγήσει σε χαμηλότερο WER.\n",
    "\n",
    "Το αντικείμενο του δεύτερου είναι να περιγράψει στατιστικώς τις λέξεις σε μορφή φωνημάτων. Η πιο γνωστή μέθοδος  είναι η GMM-HMM ( Gaussian Mixture Model-Hidden Markov model). Το HMM βασίζεται σε πεπερασμένα αυτόματα για την απότιμηση των πιθανοτήτων μετάβασης, έτσι ώστε να μπορούμε να υπολογίσουμε τις πιθανότητες των αντίστοιχων λέξεων που προκύπτουν απο τις ακολουθίες φωνημάτων. Οι HMM καταστάσεις τυπικώς συγκροτούν είτε ένα monophone είτε ένα triphone μοντέλο,όπου άλλοι μέθοδοι επίσης εφαρμόζονται για περαιτέρω βελτίωση του μοντέλου. Τα GMM μοντελοποιούν το output-παρατήρηση των HMM καταστάσεων με Gaussian μίγματα. Επομένως οι διάφορες HMM καταστάσεις θα μοντελοποιηθούν με Gaussian κατανομή μέσω ενός clustering δέντρου απόφασης στο βήμα της κατασκευής του monophone μοντέλου\n",
    "\n",
    "#### 1.\n",
    "\n",
    "Το monophone μοντέλο είναι το πρώτο κομμάτι της διαδικασίας training. Αποτελεσματικά monophone μοντέλα μπορούν να σχηματιστούν και με λίγα δεδομένα, όπως στην συγκεκριμένη περίπτωση τα 1448 του train set, κυρίως με σκοπό να δώσουν boost σε μετέπειτα μοντέλα, όπως το triphone που θα παράξουμε παρακάτω. Τα απαιτούμενα ορίσματα είναι διαρκώς κατά το training τα ακόλουθα:\n",
    "\n",
    "-Location of the accoustic : data/train\n",
    "-Location of the lexicon : data/lang_test_train_uni or data/lang_test_train_bi (Ανάλογα το μόντελο που εκπαιδεύουμε)\n",
    "-Destination Directory : exp/mono_uni or exp/mono_bi(Ανάλογα το μόντελο που εκπαιδεύουμε)\n",
    "\n",
    "Εκτελούμε την εντολή train_mono.sh τόσο για το unigram LM , όσο και για το bigram. Το αποτέλεσμα είναι η παραγωγή των ”mono_uni” και ”mono_bi”μοντέλων.\n",
    "\n",
    "Ακολοθούμε ακριβώς την ίδια διαδικασία με προηγούμενως για την κατασκευή των αντίστοιχων alignments του μοντέλου με την μόνη διαφορά ότι:\n",
    "\n",
    "Destination directory for the alignment: ”exp/mono_ali_uni” or ”exp/mono_ali_bi”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps/train_mono.sh data/train data/lang_test_train_uni exp/mono_uni\n",
      "steps/train_mono.sh: Initializing monophone system.\n",
      "steps/train_mono.sh: Compiling training graphs\n",
      "steps/train_mono.sh: Aligning data equally (pass 0)\n",
      "steps/train_mono.sh: Pass 1\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 2\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 3\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 4\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 5\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 6\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 7\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 8\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 9\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 10\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 11\n",
      "steps/train_mono.sh: Pass 12\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 13\n",
      "steps/train_mono.sh: Pass 14\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 15\n",
      "steps/train_mono.sh: Pass 16\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 17\n",
      "steps/train_mono.sh: Pass 18\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 19\n",
      "steps/train_mono.sh: Pass 20\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 21\n",
      "steps/train_mono.sh: Pass 22\n",
      "steps/train_mono.sh: Pass 23\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 24\n",
      "steps/train_mono.sh: Pass 25\n",
      "steps/train_mono.sh: Pass 26\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 27\n",
      "steps/train_mono.sh: Pass 28\n",
      "steps/train_mono.sh: Pass 29\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 30\n",
      "steps/train_mono.sh: Pass 31\n",
      "steps/train_mono.sh: Pass 32\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 33\n",
      "steps/train_mono.sh: Pass 34\n",
      "steps/train_mono.sh: Pass 35\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 36\n",
      "steps/train_mono.sh: Pass 37\n",
      "steps/train_mono.sh: Pass 38\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 39\n",
      "steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang_test_train_uni exp/mono_uni\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 48.97680763983629% of the time at utterance begin.  This may not be optimal.\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 30.695770804911323% of the time at utterance end.  This may not be optimal.\n",
      "steps/diagnostic/analyze_alignments.sh: see stats in exp/mono_uni/log/analyze_alignments.log\n",
      "3452 warnings in exp/mono_uni/log/align.*.*.log\n",
      "217 warnings in exp/mono_uni/log/acc.*.*.log\n",
      "2 warnings in exp/mono_uni/log/analyze_alignments.log\n",
      "200 warnings in exp/mono_uni/log/update.*.log\n",
      "exp/mono_uni: nj=4 align prob=-84.21 over 1.81h [retry=1.6%, fail=0.1%] states=127 gauss=999\n",
      "steps/train_mono.sh: Done training monophone system in exp/mono_uni\n",
      "steps/align_si.sh data/train data/lang_test_train_uni exp/mono_uni exp/mono_ali_uni\n",
      "steps/align_si.sh: feature type is delta\n",
      "steps/align_si.sh: aligning data in data/train using model from exp/mono_uni, putting alignments in exp/mono_ali_uni\n",
      "steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang_test_train_uni exp/mono_ali_uni\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 48.158253751705324% of the time at utterance begin.  This may not be optimal.\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 28.649386084583902% of the time at utterance end.  This may not be optimal.\n",
      "steps/diagnostic/analyze_alignments.sh: see stats in exp/mono_ali_uni/log/analyze_alignments.log\n",
      "steps/align_si.sh: done aligning data.\n",
      "steps/train_mono.sh data/train data/lang_test_train_bi exp/mono_bi\n",
      "steps/train_mono.sh: Initializing monophone system.\n",
      "steps/train_mono.sh: Compiling training graphs\n",
      "steps/train_mono.sh: Aligning data equally (pass 0)\n",
      "steps/train_mono.sh: Pass 1\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 2\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 3\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 4\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 5\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 6\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 7\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 8\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 9\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 10\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 11\n",
      "steps/train_mono.sh: Pass 12\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 13\n",
      "steps/train_mono.sh: Pass 14\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 15\n",
      "steps/train_mono.sh: Pass 16\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 17\n",
      "steps/train_mono.sh: Pass 18\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 19\n",
      "steps/train_mono.sh: Pass 20\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 21\n",
      "steps/train_mono.sh: Pass 22\n",
      "steps/train_mono.sh: Pass 23\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 24\n",
      "steps/train_mono.sh: Pass 25\n",
      "steps/train_mono.sh: Pass 26\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 27\n",
      "steps/train_mono.sh: Pass 28\n",
      "steps/train_mono.sh: Pass 29\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 30\n",
      "steps/train_mono.sh: Pass 31\n",
      "steps/train_mono.sh: Pass 32\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 33\n",
      "steps/train_mono.sh: Pass 34\n",
      "steps/train_mono.sh: Pass 35\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 36\n",
      "steps/train_mono.sh: Pass 37\n",
      "steps/train_mono.sh: Pass 38\n",
      "steps/train_mono.sh: Aligning data\n",
      "steps/train_mono.sh: Pass 39\n",
      "steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang_test_train_bi exp/mono_bi\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 48.97680763983629% of the time at utterance begin.  This may not be optimal.\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 30.695770804911323% of the time at utterance end.  This may not be optimal.\n",
      "steps/diagnostic/analyze_alignments.sh: see stats in exp/mono_bi/log/analyze_alignments.log\n",
      "2 warnings in exp/mono_bi/log/analyze_alignments.log\n",
      "3452 warnings in exp/mono_bi/log/align.*.*.log\n",
      "200 warnings in exp/mono_bi/log/update.*.log\n",
      "217 warnings in exp/mono_bi/log/acc.*.*.log\n",
      "exp/mono_bi: nj=4 align prob=-84.21 over 1.81h [retry=1.6%, fail=0.1%] states=127 gauss=999\n",
      "steps/train_mono.sh: Done training monophone system in exp/mono_bi\n",
      "steps/align_si.sh data/train data/lang_test_train_bi exp/mono_bi exp/mono_ali_bi\n",
      "steps/align_si.sh: feature type is delta\n",
      "steps/align_si.sh: aligning data in data/train using model from exp/mono_bi, putting alignments in exp/mono_ali_bi\n",
      "steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang_test_train_bi exp/mono_ali_bi\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 48.158253751705324% of the time at utterance begin.  This may not be optimal.\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 28.649386084583902% of the time at utterance end.  This may not be optimal.\n",
      "steps/diagnostic/analyze_alignments.sh: see stats in exp/mono_ali_bi/log/analyze_alignments.log\n",
      "steps/align_si.sh: done aligning data.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "#4.4.1\n",
    "source /users/savas/kaldi/egs/usc/path.sh\n",
    "cd /users/savas/kaldi/egs/usc\n",
    "\n",
    "# Train and align unigram (based) monophone accoustic model \n",
    "steps/train_mono.sh  data/train data/lang_test_train_uni exp/mono_uni\n",
    "steps/align_si.sh data/train data/lang_test_train_uni exp/mono_uni exp/mono_ali_uni \n",
    "\n",
    "#Train and align bigram (based) monophone accoustic model\n",
    "steps/train_mono.sh  data/train data/lang_test_train_bi exp/mono_bi\n",
    "steps/align_si.sh data/train data/lang_test_train_bi exp/mono_bi exp/mono_ali_bi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "Παρόμοιο FST framework, που διαδραμάτισε ρόλο κλειδί στην kaldi γραμματική, χρησιμοποίηθηκε τόσο για την training όσο για την testing διαδικασία. Το appendix C αποτελεί απεικόνιση του πως το Kaldi θα μπορούσε να παράξει transcriptions από ακουστικά features όσον αφορά την συγκεκριμένη γραμματική. Δημιουργήσαμε 6 τέτοιους γράφους έναν για κάθε μία από τις 6 γραμματικές (unigram και bigram των test, train και evaluation set) με εκτέλεση της εντολής utils/mkgraph.sh –mono. Τα αποθήκευσαμε στον φάκελο του γράφου graph_nosp_tgpr_uni και  graph_nosp_tgpr_bi για το mono_uni και mono_bi μοντέλο αντίστοιχα, με σκοπό την ανάκτηση τους σε περίπτωση που χρειαστεί.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0425213 -0.000214821\n",
      "HCLGa is not stochastic\n",
      "0.000433712 -0.00881584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tree-info exp/mono_uni/tree \n",
      "tree-info exp/mono_uni/tree \n",
      "make-h-transducer --disambig-syms-out=exp/mono_uni/graph_nosp_tgpr_uni/disambig_tid.int --transition-scale=1.0 data/lang_test_train_uni/tmp/ilabels_1_0 exp/mono_uni/tree exp/mono_uni/final.mdl \n",
      "fstrmsymbols exp/mono_uni/graph_nosp_tgpr_uni/disambig_tid.int \n",
      "fstminimizeencoded \n",
      "fstdeterminizestar --use-log=true \n",
      "fsttablecompose exp/mono_uni/graph_nosp_tgpr_uni/Ha.fst data/lang_test_train_uni/tmp/CLG_1_0.fst \n",
      "fstrmepslocal \n",
      "fstisstochastic exp/mono_uni/graph_nosp_tgpr_uni/HCLGa.fst \n",
      "add-self-loops --self-loop-scale=0.1 --reorder=true exp/mono_uni/final.mdl exp/mono_uni/graph_nosp_tgpr_uni/HCLGa.fst \n",
      "tree-info exp/mono_bi/tree \n",
      "tree-info exp/mono_bi/tree \n",
      "make-h-transducer --disambig-syms-out=exp/mono_bi/graph_nosp_tgpr_bi/disambig_tid.int --transition-scale=1.0 data/lang_test_train_bi/tmp/ilabels_1_0 exp/mono_bi/tree exp/mono_bi/final.mdl \n",
      "fstrmepslocal \n",
      "fstminimizeencoded \n",
      "fstdeterminizestar --use-log=true \n",
      "fstrmsymbols exp/mono_bi/graph_nosp_tgpr_bi/disambig_tid.int \n",
      "fsttablecompose exp/mono_bi/graph_nosp_tgpr_bi/Ha.fst data/lang_test_train_bi/tmp/CLG_1_0.fst \n",
      "fstisstochastic exp/mono_bi/graph_nosp_tgpr_bi/HCLGa.fst \n",
      "add-self-loops --self-loop-scale=0.1 --reorder=true exp/mono_bi/final.mdl exp/mono_bi/graph_nosp_tgpr_bi/HCLGa.fst \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "#4.4.2\n",
    "source /users/savas/kaldi/egs/usc/path.sh\n",
    "cd /users/savas/kaldi/egs/usc\n",
    "\n",
    "#Creation of HCLG graph based on unigram model \n",
    "utils/mkgraph.sh  data/lang_test_train_uni exp/mono_uni exp/mono_uni/graph_nosp_tgpr_uni\n",
    "\n",
    "#Creation of HCLG graph based on bigram model \n",
    "utils/mkgraph.sh  data/lang_test_train_bi exp/mono_bi exp/mono_bi/graph_nosp_tgpr_bi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "Τελικά, αφού έχουμε ολοκληρώσει το μοντέλο μας, ήρθε η ώρα να το εφαρμόσουμε στo test και evaluation set που μας δίνεται. Το μοντέλο μας, στα νέα πλέον δεδομένα, προσπαθεί να κάνει την κατάλληλη αποκωδικοποίηση και τα αποτελέσματα βρίσκονται στα αρχεία wer πoυ βρίσκονται στο path ”exp/mono_j/decode_i_j”, όπου i={dev,test} και j={uni,bi}.\n",
    "\n",
    "Εκτελούμε επαναληπτικά τον αλγόριθμο 6 φορές, μία για κάθε HCLG γράφο που δημιουργήσαμε στο προηγούμενο βήμα. Πιο συγκεκριμένα, εκτελούμε την εντολή steps/decode με –nj = 4 (αριθμός pipelines αναθετημένα σε 4 CPU πυρήνες), με ορίσματα τον γράφο-αποκωδικοποιητή και τον φάκελο data/i ,όπου i = {dev,test}, δηλαδή τα utterance συσχετιζόμενα αρχεία.\n",
    "\n",
    "Το βάρος του γλωσσικού μοντέλου για το οποίο ο αποκωδιποιητής θα ψάξει, επιβεβαιώνετε από την ”κρυφή” και ταυτόχρονη εκτέλεση του score kaldi.sh (με –scoring-opts από 1 έως 20) για τον υπολογισμό του PER \n",
    "\n",
    "Η αποκωδικοποίηση περιλαμβάνει λοιπόν wer και log αποτελέσματα στους φακέλους ”decode_dev_i” και ”decode_test_i ” του φακέλου ”mono_i” , όπου i={uni,bi}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps/decode.sh --nj 4 exp/mono_uni/graph_nosp_tgpr_uni data/dev exp/mono_uni/decode_dev_uni\n",
      "decode.sh: feature type is delta\n",
      "steps/diagnostic/analyze_lats.sh --cmd run.pl exp/mono_uni/graph_nosp_tgpr_uni exp/mono_uni/decode_dev_uni\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/mono_uni/decode_dev_uni/log/analyze_alignments.log\n",
      "Overall, lattice depth (10,50,90-percentile)=(7,39,523) and mean=282.2\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/mono_uni/decode_dev_uni/log/analyze_lattice_depth_stats.log\n",
      "local/score_kaldi.sh --cmd run.pl data/dev exp/mono_uni/graph_nosp_tgpr_uni exp/mono_uni/decode_dev_uni\n",
      "local/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0\n",
      "steps/decode.sh --nj 4 exp/mono_uni/graph_nosp_tgpr_uni data/test exp/mono_uni/decode_test_uni\n",
      "decode.sh: feature type is delta\n",
      "steps/diagnostic/analyze_lats.sh --cmd run.pl exp/mono_uni/graph_nosp_tgpr_uni exp/mono_uni/decode_test_uni\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/mono_uni/decode_test_uni/log/analyze_alignments.log\n",
      "Overall, lattice depth (10,50,90-percentile)=(7,38,484) and mean=253.3\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/mono_uni/decode_test_uni/log/analyze_lattice_depth_stats.log\n",
      "local/score_kaldi.sh --cmd run.pl data/test exp/mono_uni/graph_nosp_tgpr_uni exp/mono_uni/decode_test_uni\n",
      "local/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0\n",
      "steps/decode.sh --nj 4 exp/mono_bi/graph_nosp_tgpr_bi data/dev exp/mono_bi/decode_dev_bi\n",
      "decode.sh: feature type is delta\n",
      "steps/diagnostic/analyze_lats.sh --cmd run.pl exp/mono_bi/graph_nosp_tgpr_bi exp/mono_bi/decode_dev_bi\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 44.26229508196721% of the time at utterance begin.  This may not be optimal.\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 22.404371584699454% of the time at utterance end.  This may not be optimal.\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/mono_bi/decode_dev_bi/log/analyze_alignments.log\n",
      "Overall, lattice depth (10,50,90-percentile)=(3,18,125) and mean=62.0\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/mono_bi/decode_dev_bi/log/analyze_lattice_depth_stats.log\n",
      "local/score_kaldi.sh --cmd run.pl data/dev exp/mono_bi/graph_nosp_tgpr_bi exp/mono_bi/decode_dev_bi\n",
      "local/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0\n",
      "steps/decode.sh --nj 4 exp/mono_bi/graph_nosp_tgpr_bi data/test exp/mono_bi/decode_test_bi\n",
      "decode.sh: feature type is delta\n",
      "steps/diagnostic/analyze_lats.sh --cmd run.pl exp/mono_bi/graph_nosp_tgpr_bi exp/mono_bi/decode_test_bi\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 47.28260869565217% of the time at utterance begin.  This may not be optimal.\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 27.717391304347824% of the time at utterance end.  This may not be optimal.\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/mono_bi/decode_test_bi/log/analyze_alignments.log\n",
      "Overall, lattice depth (10,50,90-percentile)=(3,18,123) and mean=65.7\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/mono_bi/decode_test_bi/log/analyze_lattice_depth_stats.log\n",
      "local/score_kaldi.sh --cmd run.pl data/test exp/mono_bi/graph_nosp_tgpr_bi exp/mono_bi/decode_test_bi\n",
      "local/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "#4.4.3\n",
    "source /users/savas/kaldi/egs/usc/path.sh\n",
    "cd /users/savas/kaldi/egs/usc\n",
    "\n",
    "#Decode validation and test dataset using the HCLG graph created by unigram LM\n",
    "steps/decode.sh --nj 4  exp/mono_uni/graph_nosp_tgpr_uni data/dev exp/mono_uni/decode_dev_uni \n",
    "steps/decode.sh --nj 4  exp/mono_uni/graph_nosp_tgpr_uni data/test exp/mono_uni/decode_test_uni \n",
    "\n",
    "#Decode validation and test dataset using the HCLG graph created by bigram LM\n",
    "steps/decode.sh --nj 4 exp/mono_bi/graph_nosp_tgpr_bi data/dev exp/mono_bi/decode_dev_bi \n",
    "steps/decode.sh --nj 4 exp/mono_bi/graph_nosp_tgpr_bi data/test exp/mono_bi/decode_test_bi \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "Με το που τελειώνει η αποκωδικοποίηση για την δυάδα test και validation set μετά την εφαρμογή κάποιου απο τους 6 HCLG γράφους, εκτελείται η εντολή utils/best wer.sh. για εύρεση του καλύτερου δυνατού PER (δηλαδή του μικρότερου δυνατού)\n",
    "\n",
    "Παρακάτω φαίνονται τα αποτελέσματα του καλύτερο WER που πήραμε από την εκτέλεση για όλους τους γράφους HCLG, και άρα για όλα τα unigram και bigram γλωσσικά μοντέλα που κατασκευάσαμε:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER for Validation dataset based on the Unigram accoustic model :\n",
      "%WER 52.77 [ 3258 / 6174, 83 in , 1815 del, 1360  ub ] exp/mono_uni/decode_dev_uni/wer_7_0.0\n",
      "\n",
      "PER for Test dataset based on the Unigram accoustic model :\n",
      "%WER 50.44 [ 3024 / 5995, 49 in , 1690 del, 1285  ub ] exp/mono_uni/decode_te t_uni/wer_7_0.0\n",
      " \n",
      "PER for Validation dataset based on the Bigram accoustic model :\n",
      "%WER 46.06 [ 2844 / 6174, 114 in , 1230 del, 1500  ub ] exp/mono_bi/decode_dev_bi/wer_7_0.0\n",
      "\n",
      "PER for Test dataset based on the Bigram accoustic model :\n",
      "%WER 43.37 [ 2600 / 5995, 92 in , 1127 del, 1381  ub ] exp/mono_bi/decode_te t_bi/wer_7_0.0\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "#4.4.4\n",
    "source /users/savas/kaldi/egs/usc/path.sh\n",
    "cd /users/savas/kaldi/egs/usc\n",
    "\n",
    "#Printing the PER  for both unigram and bigram (based) models \n",
    "echo \"PER for Validation dataset based on the Unigram accoustic model :\"\n",
    "[ -d exp/mono_uni/decode_dev_uni ] && grep WER exp/mono_uni/decode_dev_uni/wer_* | utils/best_wer.sh\n",
    "echo\"\"\n",
    "echo \"PER for Test dataset based on the Unigram accoustic model :\"\n",
    "[ -d exp/mono_uni/decode_test_uni ] && grep WER exp/mono_uni/decode_test_uni/wer_* | utils/best_wer.sh\n",
    "echo \" \"\n",
    "echo \"PER for Validation dataset based on the Bigram accoustic model :\"\n",
    "[ -d exp/mono_bi/decode_dev_bi ] && grep WER exp/mono_bi/decode_dev_bi/wer_* | utils/best_wer.sh\n",
    "echo \"\"\n",
    "echo \"PER for Test dataset based on the Bigram accoustic model :\"\n",
    "[ -d exp/mono_bi/decode_test_bi ] && grep WER exp/mono_bi/decode_test_bi/wer_* | utils/best_wer.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ερώτημα \n",
    "\n",
    "Οι υπερπαράμετροι του scoring script για το συγκεκριμένο βήμα είναι το min και max LM-weight για lattice rescoring που πήραν τιμή 1 και 20 αντίστοιχα (το default-συνηθισμένο είναι 9 και 20)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "Το να κάνουμε train ένα triphone μοντέλο μεταφράζεται σε παραπάνω arguments για τον αριθμό των φύλλων, ή αριθμό HMM καταστάσσεων στο δέντρο απόφασης και στον αριθμό των Gaussianss.\n",
    "\n",
    "Γίνεται εκτέλεση της steps/train deltas.sh, με σκοπό να κάνουμε train τα aligments του monophone μοντέλου σε triphone, το directory του οποίου ονομάζουμε ”tri1_uni” και ”tri1_bi”, ανάλογα με το mono που έχει χρησιμοποιηθεί για την εκπαίδευση του triphone. Θα μπορούσαμε να είχαμ μία HMM κατάσταση για κάθε φώνημα, αλλά γνωρίζουμε ότι τα φωνήματα ποικίλλουν σε σημαντικό βαθμό ανάλογα με το αν βρίσκονται στην αρχή, στην μέση ή στο τέλος μίας πρότασης. Αυτό μας οδηγεί σε τουλάχιστον 168 HMM καταστάσεις μόνο για το variation. Με 2000 HMM καταστάσεις, το μοντέλο μπορεί να αποφασίσει αν είναι καλύτερο να κατανείμει μία μοναδική HMM κατάσταση σε ποιο refined αλλόφωνα του αρχικού φωνήματος. Αυτό το split των φωνημάτων αποφασίζεται από φωνητικές ερωτήσεις στο ”questions.txt” και στο ”extra questions.txt”\n",
    "\n",
    "Ο ακριβής αιρθμός φύλλων και Gaussian αποφασίζεται συχνά από ευρυστικές, ο οποίος εξαρτάται από την ποσότητα των δεδομένων, τον αριθμό των ερωτήσεων περί φωνημάτων και τον στόχο του μοντέλου. Επίσης υπάρχει ο περιορισμός ότι ο αριθμός των Gaussians πρέπει ΠΑΝΤΑ να ξεπερνάει τον αριθμό των φύλλων.  ́Οπως βλέπουμε στο script ”4 4.sh”, εμπειρικά ορίσαμε #Leaves = 2000 και #Gaussians = 10000.\n",
    "\n",
    "Μετα ακολουθείται πιστά ακριβώς η ίδια διαδικασία που περιγράφηκε στο προηγούμενο βήμα για αποκωδικοποίηση του test και evaluation set βάσει του triphone πλέον μοντέλου και εύρεση όλων των σχετικών PER που παρουσιάζονται παρακάτω:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps/train_deltas.sh 2000 10000 data/train data/lang_test_train_uni exp/mono_ali_uni exp/tri1_uni\n",
      "steps/train_deltas.sh: accumulating tree stats\n",
      "steps/train_deltas.sh: getting questions for tree-building, via clustering\n",
      "steps/train_deltas.sh: building the tree\n",
      "WARNING (gmm-init-model[5.5.585~1-7b762]:InitAmGmm():gmm-init-model.cc:55) Tree has pdf-id 1 with no stats; corresponding phone list: 6 7 8 9 10 \n",
      "** The warnings above about 'no stats' generally mean you have phones **\n",
      "** (or groups of phones) in your phone set that had no corresponding data. **\n",
      "** You should probably figure out whether something went wrong, **\n",
      "** or whether your data just doesn't happen to have examples of those **\n",
      "** phones. **\n",
      "steps/train_deltas.sh: converting alignments from exp/mono_ali_uni to use current tree\n",
      "steps/train_deltas.sh: compiling graphs of transcripts\n",
      "steps/train_deltas.sh: training pass 1\n",
      "steps/train_deltas.sh: training pass 2\n",
      "steps/train_deltas.sh: training pass 3\n",
      "steps/train_deltas.sh: training pass 4\n",
      "steps/train_deltas.sh: training pass 5\n",
      "steps/train_deltas.sh: training pass 6\n",
      "steps/train_deltas.sh: training pass 7\n",
      "steps/train_deltas.sh: training pass 8\n",
      "steps/train_deltas.sh: training pass 9\n",
      "steps/train_deltas.sh: training pass 10\n",
      "steps/train_deltas.sh: aligning data\n",
      "steps/train_deltas.sh: training pass 11\n",
      "steps/train_deltas.sh: training pass 12\n",
      "steps/train_deltas.sh: training pass 13\n",
      "steps/train_deltas.sh: training pass 14\n",
      "steps/train_deltas.sh: training pass 15\n",
      "steps/train_deltas.sh: training pass 16\n",
      "steps/train_deltas.sh: training pass 17\n",
      "steps/train_deltas.sh: training pass 18\n",
      "steps/train_deltas.sh: training pass 19\n",
      "steps/train_deltas.sh: training pass 20\n",
      "steps/train_deltas.sh: aligning data\n",
      "steps/train_deltas.sh: training pass 21\n",
      "steps/train_deltas.sh: training pass 22\n",
      "steps/train_deltas.sh: training pass 23\n",
      "steps/train_deltas.sh: training pass 24\n",
      "steps/train_deltas.sh: training pass 25\n",
      "steps/train_deltas.sh: training pass 26\n",
      "steps/train_deltas.sh: training pass 27\n",
      "steps/train_deltas.sh: training pass 28\n",
      "steps/train_deltas.sh: training pass 29\n",
      "steps/train_deltas.sh: training pass 30\n",
      "steps/train_deltas.sh: aligning data\n",
      "steps/train_deltas.sh: training pass 31\n",
      "steps/train_deltas.sh: training pass 32\n",
      "steps/train_deltas.sh: training pass 33\n",
      "steps/train_deltas.sh: training pass 34\n",
      "steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang_test_train_uni exp/tri1_uni\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 55.972696245733786% of the time at utterance begin.  This may not be optimal.\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 53.6518771331058% of the time at utterance end.  This may not be optimal.\n",
      "steps/diagnostic/analyze_alignments.sh: see stats in exp/tri1_uni/log/analyze_alignments.log\n",
      "93 warnings in exp/tri1_uni/log/acc.*.*.log\n",
      "1 warnings in exp/tri1_uni/log/build_tree.log\n",
      "74 warnings in exp/tri1_uni/log/align.*.*.log\n",
      "2 warnings in exp/tri1_uni/log/analyze_alignments.log\n",
      "69 warnings in exp/tri1_uni/log/update.*.log\n",
      "82 warnings in exp/tri1_uni/log/init_model.log\n",
      "1 warnings in exp/tri1_uni/log/questions.log\n",
      "exp/tri1_uni: nj=4 align prob=-80.24 over 1.81h [retry=1.2%, fail=0.2%] states=1040 gauss=10030 tree-impr=5.59\n",
      "steps/train_deltas.sh: Done training system with delta+delta-delta features in exp/tri1_uni\n",
      "steps/train_deltas.sh 2000 10000 data/train data/lang_test_train_bi exp/mono_ali_bi exp/tri1_bi\n",
      "steps/train_deltas.sh: accumulating tree stats\n",
      "steps/train_deltas.sh: getting questions for tree-building, via clustering\n",
      "steps/train_deltas.sh: building the tree\n",
      "WARNING (gmm-init-model[5.5.585~1-7b762]:InitAmGmm():gmm-init-model.cc:55) Tree has pdf-id 1 with no stats; corresponding phone list: 6 7 8 9 10 \n",
      "** The warnings above about 'no stats' generally mean you have phones **\n",
      "** (or groups of phones) in your phone set that had no corresponding data. **\n",
      "** You should probably figure out whether something went wrong, **\n",
      "** or whether your data just doesn't happen to have examples of those **\n",
      "** phones. **\n",
      "steps/train_deltas.sh: converting alignments from exp/mono_ali_bi to use current tree\n",
      "steps/train_deltas.sh: compiling graphs of transcripts\n",
      "steps/train_deltas.sh: training pass 1\n",
      "steps/train_deltas.sh: training pass 2\n",
      "steps/train_deltas.sh: training pass 3\n",
      "steps/train_deltas.sh: training pass 4\n",
      "steps/train_deltas.sh: training pass 5\n",
      "steps/train_deltas.sh: training pass 6\n",
      "steps/train_deltas.sh: training pass 7\n",
      "steps/train_deltas.sh: training pass 8\n",
      "steps/train_deltas.sh: training pass 9\n",
      "steps/train_deltas.sh: training pass 10\n",
      "steps/train_deltas.sh: aligning data\n",
      "steps/train_deltas.sh: training pass 11\n",
      "steps/train_deltas.sh: training pass 12\n",
      "steps/train_deltas.sh: training pass 13\n",
      "steps/train_deltas.sh: training pass 14\n",
      "steps/train_deltas.sh: training pass 15\n",
      "steps/train_deltas.sh: training pass 16\n",
      "steps/train_deltas.sh: training pass 17\n",
      "steps/train_deltas.sh: training pass 18\n",
      "steps/train_deltas.sh: training pass 19\n",
      "steps/train_deltas.sh: training pass 20\n",
      "steps/train_deltas.sh: aligning data\n",
      "steps/train_deltas.sh: training pass 21\n",
      "steps/train_deltas.sh: training pass 22\n",
      "steps/train_deltas.sh: training pass 23\n",
      "steps/train_deltas.sh: training pass 24\n",
      "steps/train_deltas.sh: training pass 25\n",
      "steps/train_deltas.sh: training pass 26\n",
      "steps/train_deltas.sh: training pass 27\n",
      "steps/train_deltas.sh: training pass 28\n",
      "steps/train_deltas.sh: training pass 29\n",
      "steps/train_deltas.sh: training pass 30\n",
      "steps/train_deltas.sh: aligning data\n",
      "steps/train_deltas.sh: training pass 31\n",
      "steps/train_deltas.sh: training pass 32\n",
      "steps/train_deltas.sh: training pass 33\n",
      "steps/train_deltas.sh: training pass 34\n",
      "steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang_test_train_bi exp/tri1_bi\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 55.972696245733786% of the time at utterance begin.  This may not be optimal.\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 53.6518771331058% of the time at utterance end.  This may not be optimal.\n",
      "steps/diagnostic/analyze_alignments.sh: see stats in exp/tri1_bi/log/analyze_alignments.log\n",
      "1 warnings in exp/tri1_bi/log/questions.log\n",
      "82 warnings in exp/tri1_bi/log/init_model.log\n",
      "74 warnings in exp/tri1_bi/log/align.*.*.log\n",
      "1 warnings in exp/tri1_bi/log/build_tree.log\n",
      "2 warnings in exp/tri1_bi/log/analyze_alignments.log\n",
      "93 warnings in exp/tri1_bi/log/acc.*.*.log\n",
      "69 warnings in exp/tri1_bi/log/update.*.log\n",
      "exp/tri1_bi: nj=4 align prob=-80.24 over 1.81h [retry=1.2%, fail=0.2%] states=1040 gauss=10030 tree-impr=5.59\n",
      "steps/train_deltas.sh: Done training system with delta+delta-delta features in exp/tri1_bi\n",
      "steps/align_si.sh data/train data/lang_test_train_uni exp/tri1_uni exp/tri1_ali_uni\n",
      "steps/align_si.sh: feature type is delta\n",
      "steps/align_si.sh: aligning data in data/train using model from exp/tri1_uni, putting alignments in exp/tri1_ali_uni\n",
      "steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang_test_train_uni exp/tri1_ali_uni\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 55.63139931740614% of the time at utterance begin.  This may not be optimal.\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 54.129692832764505% of the time at utterance end.  This may not be optimal.\n",
      "steps/diagnostic/analyze_alignments.sh: see stats in exp/tri1_ali_uni/log/analyze_alignments.log\n",
      "steps/align_si.sh: done aligning data.\n",
      "steps/align_si.sh data/train data/lang_test_train_bi exp/tri1_bi exp/tri1_ali_bi\n",
      "steps/align_si.sh: feature type is delta\n",
      "steps/align_si.sh: aligning data in data/train using model from exp/tri1_bi, putting alignments in exp/tri1_ali_bi\n",
      "steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang_test_train_bi exp/tri1_ali_bi\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 55.63139931740614% of the time at utterance begin.  This may not be optimal.\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 54.129692832764505% of the time at utterance end.  This may not be optimal.\n",
      "steps/diagnostic/analyze_alignments.sh: see stats in exp/tri1_ali_bi/log/analyze_alignments.log\n",
      "steps/align_si.sh: done aligning data.\n",
      "0.10477 -0.000458677\n",
      "HCLGa is not stochastic\n",
      "0.000451738 -0.0178664\n",
      "HCLGa is not stochastic\n",
      "steps/decode.sh --nj 4 exp/tri1_uni/graph_nosp_tgpr_uni data/dev exp/tri1_uni/decode_dev_uni\n",
      "decode.sh: feature type is delta\n",
      "steps/diagnostic/analyze_lats.sh --cmd run.pl exp/tri1_uni/graph_nosp_tgpr_uni exp/tri1_uni/decode_dev_uni\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 56.830601092896174% of the time at utterance begin.  This may not be optimal.\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 56.28415300546448% of the time at utterance end.  This may not be optimal.\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/tri1_uni/decode_dev_uni/log/analyze_alignments.log\n",
      "Overall, lattice depth (10,50,90-percentile)=(2,10,47) and mean=24.4\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/tri1_uni/decode_dev_uni/log/analyze_lattice_depth_stats.log\n",
      "local/score_kaldi.sh --cmd run.pl data/dev exp/tri1_uni/graph_nosp_tgpr_uni exp/tri1_uni/decode_dev_uni\n",
      "local/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0\n",
      "steps/decode.sh --nj 4 exp/tri1_uni/graph_nosp_tgpr_uni data/test exp/tri1_uni/decode_test_uni\n",
      "decode.sh: feature type is delta\n",
      "steps/diagnostic/analyze_lats.sh --cmd run.pl exp/tri1_uni/graph_nosp_tgpr_uni exp/tri1_uni/decode_test_uni\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 67.93478260869566% of the time at utterance begin.  This may not be optimal.\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 66.30434782608695% of the time at utterance end.  This may not be optimal.\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/tri1_uni/decode_test_uni/log/analyze_alignments.log\n",
      "Overall, lattice depth (10,50,90-percentile)=(2,10,45) and mean=21.5\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/tri1_uni/decode_test_uni/log/analyze_lattice_depth_stats.log\n",
      "local/score_kaldi.sh --cmd run.pl data/test exp/tri1_uni/graph_nosp_tgpr_uni exp/tri1_uni/decode_test_uni\n",
      "local/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0\n",
      "steps/decode.sh --nj 4 exp/tri1_bi/graph_nosp_tgpr_bi data/dev exp/tri1_bi/decode_dev_bi\n",
      "decode.sh: feature type is delta\n",
      "steps/diagnostic/analyze_lats.sh --cmd run.pl exp/tri1_bi/graph_nosp_tgpr_bi exp/tri1_bi/decode_dev_bi\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 52.459016393442624% of the time at utterance begin.  This may not be optimal.\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 50.81967213114754% of the time at utterance end.  This may not be optimal.\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/tri1_bi/decode_dev_bi/log/analyze_alignments.log\n",
      "Overall, lattice depth (10,50,90-percentile)=(2,7,31) and mean=13.6\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/tri1_bi/decode_dev_bi/log/analyze_lattice_depth_stats.log\n",
      "local/score_kaldi.sh --cmd run.pl data/dev exp/tri1_bi/graph_nosp_tgpr_bi exp/tri1_bi/decode_dev_bi\n",
      "local/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0\n",
      "steps/decode.sh --nj 4 exp/tri1_bi/graph_nosp_tgpr_bi data/test exp/tri1_bi/decode_test_bi\n",
      "decode.sh: feature type is delta\n",
      "steps/diagnostic/analyze_lats.sh --cmd run.pl exp/tri1_bi/graph_nosp_tgpr_bi exp/tri1_bi/decode_test_bi\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 58.15217391304348% of the time at utterance begin.  This may not be optimal.\n",
      "analyze_phone_length_stats.py: WARNING: optional-silence sil is seen only 59.78260869565217% of the time at utterance end.  This may not be optimal.\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/tri1_bi/decode_test_bi/log/analyze_alignments.log\n",
      "Overall, lattice depth (10,50,90-percentile)=(1,7,30) and mean=13.0\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/tri1_bi/decode_test_bi/log/analyze_lattice_depth_stats.log\n",
      "local/score_kaldi.sh --cmd run.pl data/test exp/tri1_bi/graph_nosp_tgpr_bi exp/tri1_bi/decode_test_bi\n",
      "local/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tree-info exp/tri1_uni/tree \n",
      "tree-info exp/tri1_uni/tree \n",
      "make-h-transducer --disambig-syms-out=exp/tri1_uni/graph_nosp_tgpr_uni/disambig_tid.int --transition-scale=1.0 data/lang_test_train_uni/tmp/ilabels_3_1 exp/tri1_uni/tree exp/tri1_uni/final.mdl \n",
      "fsttablecompose exp/tri1_uni/graph_nosp_tgpr_uni/Ha.fst data/lang_test_train_uni/tmp/CLG_3_1.fst \n",
      "fstminimizeencoded \n",
      "fstrmsymbols exp/tri1_uni/graph_nosp_tgpr_uni/disambig_tid.int \n",
      "fstdeterminizestar --use-log=true \n",
      "fstrmepslocal \n",
      "fstisstochastic exp/tri1_uni/graph_nosp_tgpr_uni/HCLGa.fst \n",
      "add-self-loops --self-loop-scale=0.1 --reorder=true exp/tri1_uni/final.mdl exp/tri1_uni/graph_nosp_tgpr_uni/HCLGa.fst \n",
      "tree-info exp/tri1_bi/tree \n",
      "tree-info exp/tri1_bi/tree \n",
      "make-h-transducer --disambig-syms-out=exp/tri1_bi/graph_nosp_tgpr_bi/disambig_tid.int --transition-scale=1.0 data/lang_test_train_bi/tmp/ilabels_3_1 exp/tri1_bi/tree exp/tri1_bi/final.mdl \n",
      "fstrmepslocal \n",
      "fstminimizeencoded \n",
      "fstrmsymbols exp/tri1_bi/graph_nosp_tgpr_bi/disambig_tid.int \n",
      "fsttablecompose exp/tri1_bi/graph_nosp_tgpr_bi/Ha.fst data/lang_test_train_bi/tmp/CLG_3_1.fst \n",
      "fstdeterminizestar --use-log=true \n",
      "fstisstochastic exp/tri1_bi/graph_nosp_tgpr_bi/HCLGa.fst \n",
      "add-self-loops --self-loop-scale=0.1 --reorder=true exp/tri1_bi/final.mdl exp/tri1_bi/graph_nosp_tgpr_bi/HCLGa.fst \n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "#4.4.5\n",
    "source /users/savas/kaldi/egs/usc/path.sh\n",
    "cd /users/savas/kaldi/egs/usc\n",
    "\n",
    "#Computes training alignments using a model with  delta+delta-delta features features.\n",
    "\n",
    "steps/train_deltas.sh 2000 10000 data/train data/lang_test_train_uni exp/mono_ali_uni exp/tri1_uni \n",
    "steps/train_deltas.sh 2000 10000 data/train data/lang_test_train_bi exp/mono_ali_bi exp/tri1_bi \n",
    "\n",
    "steps/align_si.sh data/train data/lang_test_train_uni exp/tri1_uni exp/tri1_ali_uni\n",
    "steps/align_si.sh data/train data/lang_test_train_bi exp/tri1_bi exp/tri1_ali_bi\n",
    "\n",
    "#NumLeaves = 2000: it can recognize all phone sequences, the numleaves determines \n",
    "#how many classes it splits them into for purposes of pooling similar \n",
    "#contexts. \n",
    "#NumGauss = 10000: The number of Gaussians is the total over all leaves, so the average \n",
    "#num-gauss per pdf/leaf is num-gauss/num-leaves, but leaves with more \n",
    "#data will get somewhat more Gaussians. \n",
    "\n",
    "#=============Make AGAIN the testing with the new trained model=================\n",
    "#Decoding again Validation, test with the new trained model\n",
    "#FEATURE TYPE IS DELTA\n",
    "\n",
    "utils/mkgraph.sh data/lang_test_train_uni exp/tri1_uni exp/tri1_uni/graph_nosp_tgpr_uni \n",
    "utils/mkgraph.sh data/lang_test_train_bi exp/tri1_bi exp/tri1_bi/graph_nosp_tgpr_bi \n",
    "\n",
    "steps/decode.sh --nj 4  exp/tri1_uni/graph_nosp_tgpr_uni data/dev exp/tri1_uni/decode_dev_uni \n",
    "steps/decode.sh --nj 4  exp/tri1_uni/graph_nosp_tgpr_uni data/test exp/tri1_uni/decode_test_uni\n",
    "\n",
    "steps/decode.sh --nj 4  exp/tri1_bi/graph_nosp_tgpr_bi data/dev exp/tri1_bi/decode_dev_bi\n",
    "steps/decode.sh --nj 4  exp/tri1_bi/graph_nosp_tgpr_bi data/test exp/tri1_bi/decode_test_bi  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER for Validation dataset based on the Unigram triphone accoustic model :\n",
      "%WER 39.25 [ 2423 / 6174, 268 in , 765 del, 1390  ub ] exp/tri1_uni/decode_dev_uni/wer_8_0.0\n",
      "\n",
      "PER for Test dataset based on the Unigram triphone accoustic model :\n",
      "%WER 35.95 [ 2155 / 5995, 221 in , 711 del, 1223  ub ] exp/tri1_uni/decode_te t_uni/wer_7_0.5\n",
      "\n",
      "PER for Validation dataset based on the Bigram triphone accoustic model :\n",
      "%WER 34.76 [ 2146 / 6174, 232 in , 582 del, 1332  ub ] exp/tri1_bi/decode_dev_bi/wer_9_0.0\n",
      "\n",
      "PER for Test dataset based on the Bigram triphone accoustic model :\n",
      "%WER 31.38 [ 1881 / 5995, 146 in , 611 del, 1124  ub ] exp/tri1_bi/decode_te t_bi/wer_10_0.5\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "source /users/savas/kaldi/egs/usc/path.sh\n",
    "cd /users/savas/kaldi/egs/usc\n",
    "\n",
    "echo \"PER for Validation dataset based on the Unigram triphone accoustic model :\"\n",
    "[ -d exp/tri1_uni/decode_dev_uni ] && grep WER exp/tri1_uni/decode_dev_uni/wer_* | utils/best_wer.sh\n",
    "echo\"\"\n",
    "echo \"PER for Test dataset based on the Unigram triphone accoustic model :\"\n",
    "[ -d exp/tri1_uni/decode_test_uni ] && grep WER exp/tri1_uni/decode_test_uni/wer_* | utils/best_wer.sh\n",
    "echo \"\"\n",
    "echo \"PER for Validation dataset based on the Bigram triphone accoustic model :\"\n",
    "[ -d exp/tri1_bi/decode_dev_bi ] && grep WER exp/tri1_bi/decode_dev_bi/wer_* | utils/best_wer.sh\n",
    "echo\"\"\n",
    "echo \"PER for Test dataset based on the Bigram triphone accoustic model :\"\n",
    "[ -d exp/tri1_bi/decode_test_bi ] && grep WER exp/tri1_bi/decode_test_bi/wer_* | utils/best_wer.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ερώτημα 4 \n",
    "\n",
    "Σκοπός ενός ακουστικού μοντέλου είναι να περιγράψει στατιστικά τις λέξεις ενός λεξιλογίου μέσω ακολουθιών φωνημάτων. Το μοντέλο GMM-HMM είναι ένα γνωστό μοντέλο αρκετά αποτελεσματικό. Το κομμάτι των HMM(Hidden Markov Model) είναι βασισμένο σε ένα αυτόματο πεπερασμένων καταστάσεων το οποίο αναπαριστά τις πιθανότητες μετάβασης φωνημάτων σε λέξεις .Το μοντέλο αυτό καταλήγει να αναπαριστά την πιθανότητα εμφάνισης λέξεων μέσω των ακολουθιών από διακριτά φωνήματα.\n",
    "\n",
    "Οι πιο γνωστές αρχιτεκτονικές HMMs για αναγνώριση φωνής συνισ τώνται από 3 κατασ τάσεις όπως παρουσιάζεται δίπλα στις λευκές καταστάσεις. Η λειτουργία του GMM έχει σκοπό να προσδιορίσει πάνω στο αυτό- ματο και να ομαδοποιήσει καταστάσεις σε συγκεκριμένες υποκατηγορίες το οποίο λειτουργεί κάτι σαν χαρτογράφηση περιοχών πάνω στο αυτόματο του ΗΜΜ. Για παράδειγμα θα μπορεί να προσδιορίσει ένα φώνημα που εμφανίζεται σε πολλές λέξεις. Η εκπαίδευση αυτού του μοντέλου γίνεται επαναληπτικά πάνω σε ένα train set το οποίο βοηθά το GMM με βάσει τα αποτελέσματα του HMM για τις νέες εισόδους που δέχεται να κάνει ταξινόμηση σε κλάσεις.\n",
    "\n",
    "\n",
    "\n",
    "## Ερώτημα 5\n",
    "\n",
    "Η πιθανότητα posteriori υπολογίζεται με τον ακόλουθο τύπο,όπου X είναι ένα διάνυσμα παρατηρήσεων(features) που έχει προκύψει από το ακουστικό μοντέλο βάσει των οποίων υπολογίζεται ποια είναι η πιθανότητα της λέξης W δεδομένου του Χ.\n",
    "$$P(W|X) = \\frac{P(W)P(X|W)}{P(X)}$$\n",
    "\n",
    "Για να βρούμε την πιο πιθανή λέξη (ή φώνημα ) στην δική μας περίπτωση θα χρησιμοποιήσουμε την συνάρτηση argmax πάνω στο σύνολο των λέξεων (ή φωνημάτων) και ο τύπος θα έχει την μορφή\n",
    "\n",
    "$$W_{most_{pos}} = argmax_{_{_{W∈ω}}}P(W|X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ερώτημα 6\n",
    "\n",
    "Γενικά υπάρχουν τέσσερα επίπεδα στον γράφο-αποκωδικοποιητή και μια σύνθεση τεσσάτων γράφων-συστατικών ήταν αναμενόμενη για την κατασκευή του τελικού γράφου HCLG:\n",
    "• H - mapping από HMM μεταβάσεις σε context-dependent labels\n",
    "• C - mapping από context-dependent labels σε φωνήματα\n",
    "• L - mapping από φωνήματα σε λέξεις\n",
    "• G - το γλωσσσικό μοντέλο (input και output είναι λέξεις)\n",
    "Η γραμματική ουσιαστικά χρησιμοποιείται και στην φάση του training και στην φάση του testing. Το παράρτημα C με το περιεχόμενο των λέξεων αναπαριστά ένα παράδειγμα για το πως το KALDI μοντελοποιεί την μετάβαση από το ακουστικό μοντέλο σύμφωνα με την γραμματική του."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
